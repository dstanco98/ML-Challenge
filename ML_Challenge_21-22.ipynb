{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2021/2022 - Challenge \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourNameSurname='Donato Francesco Pio Stanco' # e.g., yourNameSurname='Mario Rossi'\n",
    "yourMatricolaNumber='2027523' # e.g., yourMatricolaNumber='12345678'\n",
    "yourStudentEMAIL='stanco.2027523@studenti.uniroma1.it' # e.g., yourStudentEMAIL='rossim.12345678@studenti.uniroma1.it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Mandatory Rules:\n",
    "- This year the results of the challenges will count 11,2/28 of your final grade (full info about grades <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto '>here</a>).\n",
    "- Only one submission is allowed. We will not consider multiple submissions.\n",
    "- Please remember your solution must be <b>\"YOUR SOLUTION\"</b>, hence you are requested to deliver your individual answers/arguments/opinions/critics.\n",
    "- Mail your solution (with your <b>jupyter notebook</b> and the cleaned dataset) only to stefano.faralli@uniroma1.it <b>deadlines are announced on the ML google group and <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto'>here</a> (NO EXCEPTIONS)</b> if you miss to deliver your solution you must wait the next (if any) available deadline. \n",
    "- The subject of your email must be: \"[ML-21-22-Challenge_solution] NAME - SURNAME - MATRICOLA\".\n",
    "- Double check the subject of your email and the attachments.\n",
    "- In case you want to compress the attachment, <b>USE ONLY STANDARD ZIP compression</b> (NO RAR,7Z etc..).\n",
    "- <b>Please sumbit The notebook (with SAVED OUTPUTS) and the cleaned dataset!</b>.\n",
    "- Your solution might be considered as the \"copy\" of others solutions, in that specific case the resulting score for all involved students will be 0/8.\n",
    "- Then read carefully all the part of the jupyter notebook and fill all fields.\n",
    "- <b>solutions (and correspondig points) are evaluated mainly on your thoughts/comments/opinions</b>.  \n",
    "- If you have questions <b>Don't write \"personal\" emails</b> to Stefano Faralli, instead <b>use our google group</b>.\n",
    "- A solution having a summary discussion with less than 500 words is evaluated with 0 points.\n",
    "- Comments summary etc.. must be in <b>English</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "### Dataset and task Description:\n",
    "<img width='400' src='news-online.jpeg'/>\n",
    "\n",
    "- The challenge is about online news popularity;\n",
    "- The provided dataset consists of one single csv file (\"OnlineNewsPopularity.csv\");\n",
    "- The provided dataset is a modified <b>noisy</b> version of the original dataset described in [1];\n",
    "\n",
    "[1] K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\n",
    "    Support System for Predicting the Popularity of Online News. Proceedings\n",
    "    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\n",
    "    September, Coimbra, Portugal\n",
    "\n",
    "\n",
    "This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal of the task is to predict the number of shares in social networks (popularity).\n",
    "\n",
    "Number of Instances: <b>39,797</b> \n",
    "\n",
    "Number of Attributes: <b>61</b>\n",
    "\n",
    "Target: <b>shares</b>\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "<table>\n",
    " <tr><th> index </th><th>name</th><th>description</th></tr>\n",
    " <tr><td>0</td><td>url</td><td>URL of the article</td></tr>\n",
    " <tr><td>1</td><td>timedelta</td><td>Days between the article publication and the dataset acquisition</td></tr>\n",
    " <tr><td>2</td><td>n_tokens_title</td><td>Number of words in the title</td></tr>\n",
    " <tr><td>3</td><td>n_tokens_content</td><td>Number of words in the content</td></tr>\n",
    " <tr><td>4</td><td>n_unique_tokens</td><td>Rate of unique words in the content</td></tr>\n",
    " <tr><td>5</td><td>n_non_stop_words</td><td>Rate of non-stop words in the content</td></tr>\n",
    " <tr><td>6</td><td>n_non_stop_unique_tokens</td><td>Rate of unique non-stop words in content</td></tr>\n",
    " <tr><td>7</td><td>num_hrefs</td><td>Number of links</td></tr>\n",
    " <tr><td>8</td><td>num_self_hrefs</td><td>Number of links to other articles published by Mashable</td></tr>\n",
    " <tr><td>9</td><td>num_imgs</td><td>Number of images</td></tr>\n",
    " <tr><td>10</td><td>num_videos</td><td>Number of videos</td></tr>\n",
    " <tr><td>11</td><td>average_token_length</td><td>Average length of the words in the content</td></tr>\n",
    " <tr><td>12</td><td>num_keywords</td><td>Number of keywords in the metadata</td></tr>\n",
    " <tr><td>13</td><td>data_channel_is_lifestyle</td><td>Is data channel 'Lifestyle'?</td></tr>\n",
    " <tr><td>14</td><td>data_channel_is_entertainment</td><td>Is data channel 'Entertainment'?</td></tr>\n",
    " <tr><td>15</td><td>data_channel_is_bus</td><td>Is data channel 'Business'?</td></tr>\n",
    " <tr><td>16</td><td>data_channel_is_socmed</td><td>Is data channel 'Social Media'?</td></tr>\n",
    " <tr><td>17</td><td>data_channel_is_tech</td><td>Is data channel 'Tech'?</td></tr>\n",
    " <tr><td>18</td><td>data_channel_is_world</td><td>Is data channel 'World'?</td></tr>\n",
    " <tr><td>19</td><td>kw_min_min</td><td>Worst keyword (min. shares)</td></tr>\n",
    " <tr><td>20</td><td>kw_max_min</td><td>Worst keyword (max. shares)</td></tr>\n",
    " <tr><td>21</td><td>kw_avg_min</td><td>Worst keyword (avg. shares)</td></tr>\n",
    " <tr><td>22</td><td>kw_min_max</td><td>Best keyword (min. shares)</td></tr>\n",
    " <tr><td>23</td><td>kw_max_max</td><td>Best keyword (max. shares)</td></tr>\n",
    " <tr><td>24</td><td>kw_avg_max</td><td>Best keyword (avg. shares)</td></tr>\n",
    " <tr><td>25</td><td>kw_min_avg</td><td>Avg. keyword (min. shares)</td></tr>\n",
    " <tr><td>26</td><td>kw_max_avg</td><td>Avg. keyword (max. shares)</td></tr>\n",
    " <tr><td>27</td><td>kw_avg_avg</td><td>Avg. keyword (avg. shares)</td></tr>\n",
    " <tr><td>28</td><td>self_reference_min_shares</td><td>Min. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>29</td><td>self_reference_max_shares</td><td>Max. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>30</td><td>self_reference_avg_sharess</td><td>Avg. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>31</td><td>weekday_is_monday</td><td>Was the article published on a Monday?</td></tr>\n",
    " <tr><td>32</td><td>weekday_is_tuesday</td><td>Was the article published on a Tuesday?</td></tr>\n",
    " <tr><td>33</td><td>weekday_is_wednesday</td><td>Was the article published on a Wednesday?</td></tr>\n",
    " <tr><td>34</td><td>weekday_is_thursday</td><td>Was the article published on a Thursday?</td></tr>\n",
    " <tr><td>35</td><td>weekday_is_friday</td><td>Was the article published on a Friday?</td></tr>\n",
    " <tr><td>36</td><td>weekday_is_saturday</td><td>Was the article published on a Saturday?</td></tr>\n",
    " <tr><td>37</td><td>weekday_is_sunday</td><td> Was the article published on a Sunday?</td></tr>\n",
    " <tr><td>38</td><td>is_weekend</td><td>Was the article published on the weekend?</td></tr>\n",
    " <tr><td>39</td><td>LDA_00</td><td>Closeness to LDA topic 0</td></tr>\n",
    " <tr><td>40</td><td>LDA_01</td><td>Closeness to LDA topic 1</td></tr>\n",
    " <tr><td>41</td><td>LDA_02</td><td>Closeness to LDA topic 2</td></tr>\n",
    " <tr><td>42</td><td>LDA_03</td><td>Closeness to LDA topic 3</td></tr>\n",
    " <tr><td>43</td><td>LDA_04</td><td>Closeness to LDA topic 4</td></tr>\n",
    " <tr><td>44</td><td>global_subjectivity</td><td>Text subjectivity</td></tr>\n",
    " <tr><td>45</td><td>global_sentiment_polarity</td><td>Text sentiment polarity</td></tr>\n",
    " <tr><td>46</td><td>global_rate_positive_words</td><td>Rate of positive words in the content</td></tr>\n",
    " <tr><td>47</td><td>global_rate_negative_words</td><td> Rate of negative words in the content</td></tr>\n",
    " <tr><td>48</td><td>rate_positive_words</td><td>Rate of positive words among non-neutral tokens</td></tr>\n",
    " <tr><td>49</td><td>rate_negative_words</td><td>Rate of negative words among non-neutral tokens</td></tr>\n",
    " <tr><td>50</td><td>avg_positive_polarity</td><td>Avg. polarity of positive words</td></tr>\n",
    " <tr><td>51</td><td>min_positive_polarity</td><td>Min. polarity of positive words</td></tr>\n",
    " <tr><td>52</td><td>max_positive_polarity</td><td>Max. polarity of positive words</td></tr>\n",
    " <tr><td>53</td><td>avg_negative_polarity</td><td>Avg. polarity of negative words</td></tr>\n",
    " <tr><td>54</td><td>min_negative_polarity</td><td>Min. polarity of negative words</td></tr>\n",
    " <tr><td>55</td><td>max_negative_polarity</td><td>Max. polarity of negative words</td></tr>\n",
    " <tr><td>56</td><td>title_subjectivity</td><td>Title subjectivity</td></tr>\n",
    " <tr><td>57</td><td>title_sentiment_polarity</td><td>Title polarity</td></tr>\n",
    " <tr><td>58</td><td>abs_title_subjectivity</td><td>Absolute subjectivity level</td></tr>\n",
    " <tr><td>59</td><td>abs_title_sentiment_polarity</td><td>Absolute polarity level</td></tr>\n",
    "     <tr><td>60</td><td>shares</td><td>Number of shares (target)</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 2. Pre-processing (up to 3 of 11.2 points)     \n",
    "     \n",
    "     \n",
    "## 2.1 Clean and Load the Dataset (up to 1 of 11.2 points)\n",
    "Use the following two cells (a code cell and, a markdown cell) to: \n",
    "- create a pandas DataFrame by loading a cleaned version of the \"OnlineNewsPopularity.cvs\" file.  \n",
    "- describe the identified noise and the methodology used to fix the problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import all the necessary libreries and function that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The next import is necessary to do the pre-processing phase\n",
    "from sklearn import preprocessing\n",
    "# We import the simple imputer algorithm\n",
    "from sklearn.impute import SimpleImputer\n",
    "# We import the function SMOTE to use the SMOTE technique\n",
    "from imblearn.over_sampling import SMOTE \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "# We import the function train_test_split that splits the pandas dataframe in 2 parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We import the functions necessary for the hyper parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# We import the necessary for Decision Tree Classifier\n",
    "from sklearn import tree\n",
    "# We import the necessary for Support Vector Machine Learning\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# We import the functions necessary to show the results of our models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# We import the function to do the cross-validation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I opened the csv file using Visual Studio Code and found the first noisy data which are characters of \\n (newline character), in row 39649, 39646, 39593, 36264, so I decided to delete them and format the rows correctly. Then I used the read_csv function but it gives me an error \"at line 39536 expected 61 fields, saw 62\" and in the end I found a noisy ',' in this row so I decide to delete it.\n",
    "Analyzing the dataset I found noisy strings ' n.a.' that I deduced that they represented missing values so I used a parameter of the function read_csv, that is na_values parameter, that allows to replace it with NaN values.\\\n",
    "Also as specified in the delivery I loaded a cleaned version of the dataset called <b>\"CleanedOnlineNewsPopularity\"</b> using the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>http://mashable.com/2014/12/27/samsung-app-aut...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>http://mashable.com/2014/12/27/seth-rogen-jame...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>http://mashable.com/2014/12/27/son-pays-off-mo...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>http://mashable.com/2014/12/27/ukraine-blasts/</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>http://mashable.com/2014/12/27/youtube-channel...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url   timedelta  \\\n",
       "0      http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1      http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2      http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3      http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4       http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "...                                                  ...         ...   \n",
       "39639  http://mashable.com/2014/12/27/samsung-app-aut...         8.0   \n",
       "39640  http://mashable.com/2014/12/27/seth-rogen-jame...         8.0   \n",
       "39641  http://mashable.com/2014/12/27/son-pays-off-mo...         8.0   \n",
       "39642     http://mashable.com/2014/12/27/ukraine-blasts/         8.0   \n",
       "39643  http://mashable.com/2014/12/27/youtube-channel...         8.0   \n",
       "\n",
       "        n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0                 12.0              219.0          0.663594   \n",
       "1                  9.0              255.0          0.604743   \n",
       "2                  9.0              211.0          0.575130   \n",
       "3                  9.0              531.0          0.503788   \n",
       "4                 13.0             1072.0          0.415646   \n",
       "...                ...                ...               ...   \n",
       "39639             11.0              346.0          0.529052   \n",
       "39640             12.0              328.0          0.696296   \n",
       "39641             10.0              442.0          0.516355   \n",
       "39642              6.0              682.0          0.539493   \n",
       "39643             10.0              157.0          0.701987   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens   num_hrefs  \\\n",
       "0                    1.0                   0.815385         4.0   \n",
       "1                    1.0                   0.791946         3.0   \n",
       "2                    1.0                   0.663866         3.0   \n",
       "3                    1.0                   0.665635         9.0   \n",
       "4                    1.0                   0.540890        19.0   \n",
       "...                  ...                        ...         ...   \n",
       "39639                1.0                   0.684783         9.0   \n",
       "39640                1.0                   0.885057         9.0   \n",
       "39641                1.0                   0.644128        24.0   \n",
       "39642                1.0                   0.692661        10.0   \n",
       "39643                1.0                   0.846154         1.0   \n",
       "\n",
       "        num_self_hrefs   num_imgs  ...   min_positive_polarity  \\\n",
       "0                  2.0        1.0  ...                0.100000   \n",
       "1                  1.0        1.0  ...                0.033333   \n",
       "2                  1.0        1.0  ...                0.100000   \n",
       "3                  0.0        1.0  ...                0.136364   \n",
       "4                 19.0       20.0  ...                0.033333   \n",
       "...                ...        ...  ...                     ...   \n",
       "39639              7.0        1.0  ...                0.100000   \n",
       "39640              7.0        3.0  ...                0.136364   \n",
       "39641              1.0       12.0  ...                0.136364   \n",
       "39642              1.0        1.0  ...                0.062500   \n",
       "39643              1.0        0.0  ...                0.100000   \n",
       "\n",
       "        max_positive_polarity   avg_negative_polarity   min_negative_polarity  \\\n",
       "0                        0.70               -0.350000                  -0.600   \n",
       "1                        0.70               -0.118750                  -0.125   \n",
       "2                        1.00               -0.466667                  -0.800   \n",
       "3                        0.80               -0.369697                  -0.600   \n",
       "4                        1.00               -0.220192                  -0.500   \n",
       "...                       ...                     ...                     ...   \n",
       "39639                    0.75               -0.260000                  -0.500   \n",
       "39640                    0.70               -0.211111                  -0.400   \n",
       "39641                    0.50               -0.356439                  -0.800   \n",
       "39642                    0.50               -0.205246                  -0.500   \n",
       "39643                    0.50               -0.200000                  -0.200   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500    593.0  \n",
       "1                     0.500000                       0.000000    711.0  \n",
       "2                     0.500000                       0.000000   1500.0  \n",
       "3                     0.500000                       0.000000   1200.0  \n",
       "4                     0.045455                       0.136364    505.0  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000   1800.0  \n",
       "39640                 0.200000                       1.000000   1900.0  \n",
       "39641                 0.045455                       0.136364   1900.0  \n",
       "39642                 0.500000                       0.000000   1100.0  \n",
       "39643                 0.166667                       0.250000   1300.0  \n",
       "\n",
       "[39644 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write here the code for 2.1 Load the Dataset\n",
    "# This code is used to load the csv file CleanOnlineNewsPopularity.csv, we use read_csv function of pandas\n",
    "online_news_popularity = pd.read_csv('CleanOnlineNewsPopularity.csv', na_values = [\" n.a.\"])\n",
    "online_news_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use describe function to see statistical info about the features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39643.000000</td>\n",
       "      <td>39642.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39643.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544257</td>\n",
       "      <td>1.249937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.447822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309507</td>\n",
       "      <td>4.107949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11627.089598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "count  39644.000000     39644.000000       39644.000000      39644.000000   \n",
       "mean     354.530471        10.398749         546.514731          0.548216   \n",
       "std      214.163767         2.114037         471.107508          3.520708   \n",
       "min        8.000000         2.000000           0.000000          0.000000   \n",
       "25%      164.000000         9.000000         246.000000          0.470870   \n",
       "50%      339.000000        10.000000         409.000000          0.539226   \n",
       "75%      542.000000        12.000000         716.000000          0.608696   \n",
       "max      731.000000        23.000000        8474.000000        701.000000   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count       39644.000000               39644.000000  39644.000000   \n",
       "mean            0.996469                   0.689175     10.883690   \n",
       "std             5.231231                   3.264816     11.332017   \n",
       "min             0.000000                   0.000000      0.000000   \n",
       "25%             1.000000                   0.625739      4.000000   \n",
       "50%             1.000000                   0.690476      8.000000   \n",
       "75%             1.000000                   0.754630     14.000000   \n",
       "max          1042.000000                 650.000000    304.000000   \n",
       "\n",
       "        num_self_hrefs      num_imgs    num_videos  ...  \\\n",
       "count     39644.000000  39643.000000  39642.000000  ...   \n",
       "mean          3.293638      4.544257      1.249937  ...   \n",
       "std           3.855141      8.309507      4.107949  ...   \n",
       "min           0.000000      0.000000      0.000000  ...   \n",
       "25%           1.000000      1.000000      0.000000  ...   \n",
       "50%           3.000000      1.000000      0.000000  ...   \n",
       "75%           4.000000      4.000000      1.000000  ...   \n",
       "max         116.000000    128.000000     91.000000  ...   \n",
       "\n",
       "        min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "count            39644.000000            39644.000000            39644.000000   \n",
       "mean                 0.095446                0.756728               -0.259524   \n",
       "std                  0.071315                0.247786                0.127726   \n",
       "min                  0.000000                0.000000               -1.000000   \n",
       "25%                  0.050000                0.600000               -0.328383   \n",
       "50%                  0.100000                0.800000               -0.253333   \n",
       "75%                  0.100000                1.000000               -0.186905   \n",
       "max                  1.000000                1.000000                0.000000   \n",
       "\n",
       "        min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "count            39644.000000            39644.000000         39644.000000   \n",
       "mean                -0.521944               -0.107500             0.282353   \n",
       "std                  0.290290                0.095373             0.324247   \n",
       "min                 -1.000000               -1.000000             0.000000   \n",
       "25%                 -0.700000               -0.125000             0.000000   \n",
       "50%                 -0.500000               -0.100000             0.150000   \n",
       "75%                 -0.300000               -0.050000             0.500000   \n",
       "max                  0.000000                0.000000             1.000000   \n",
       "\n",
       "        title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "count               39644.000000             39644.000000   \n",
       "mean                    0.071425                 0.341843   \n",
       "std                     0.265450                 0.188791   \n",
       "min                    -1.000000                 0.000000   \n",
       "25%                     0.000000                 0.166667   \n",
       "50%                     0.000000                 0.500000   \n",
       "75%                     0.150000                 0.500000   \n",
       "max                     1.000000                 0.500000   \n",
       "\n",
       "        abs_title_sentiment_polarity         shares  \n",
       "count                   39644.000000   39643.000000  \n",
       "mean                        0.156064    3395.447822  \n",
       "std                         0.226294   11627.089598  \n",
       "min                         0.000000       1.000000  \n",
       "25%                         0.000000     946.000000  \n",
       "50%                         0.000000    1400.000000  \n",
       "75%                         0.250000    2800.000000  \n",
       "max                         1.000000  843300.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing statistical info about dataset\n",
    "online_news_popularity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments for step 2.1 Clean and Load the Dataset\n",
    "\n",
    "### Comments 2.1\n",
    "For this step I loaded the dataset and started a first cleanup phase of the dataset which will continue in sections 2.3 (Fearure Importance) and 3 (Model Selection). The key step to complete this section was to identify the noisy data and missing values and replace and delete them as best as possible. To do this I used what was provided by the pandas library and its functions, using all the necessary parameters of the various functions such as for example the read_csv function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Analysis (up to 1 of 11.2 points)\n",
    "In the following code cell (feel free to create new cells), remember to comment your code snippets:\n",
    "\n",
    "1) Print the total number of samples;\n",
    "\n",
    "2) Print a table with the first 15 samples;\n",
    "\n",
    "3) Plot the histogram distribution of \"shares\";\n",
    "\n",
    "4) A bar chart counting the attributes:  data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Print the total number of samples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39644, 61)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write here the code for 2.2 Dataset Analysis\n",
    "# The first code shows the \"shape\" of the CSV file (i.e. the shape of samples)\n",
    "online_news_popularity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39644"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This second code just show the lenght of the Pandas DataFrame\n",
    "len(online_news_popularity.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Print a table with the first 15 samples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://mashable.com/2013/01/07/beewi-smart-toys/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.195000</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://mashable.com/2013/01/07/bodymedia-armba...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>0.418163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549834</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.224479</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>556.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://mashable.com/2013/01/07/canon-poweshot-n/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>0.433574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572108</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.242778</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://mashable.com/2013/01/07/car-of-the-futu...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://mashable.com/2013/01/07/chuck-hagel-web...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.238095</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://mashable.com/2013/01/07/cosmic-events-d...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731638</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.415064</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://mashable.com/2013/01/07/crayon-creatures/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.262500</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://mashable.com/2013/01/07/creature-cups/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707602</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.310417</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://mashable.com/2013/01/07/dad-jokes/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841530</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.337889</td>\n",
       "      <td>-0.7000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://mashable.com/2013/01/07/downton-abbey-t...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.562753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.138690</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>761.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url   timedelta  \\\n",
       "0   http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1   http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2   http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3   http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4    http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "5    http://mashable.com/2013/01/07/beewi-smart-toys/       731.0   \n",
       "6   http://mashable.com/2013/01/07/bodymedia-armba...       731.0   \n",
       "7    http://mashable.com/2013/01/07/canon-poweshot-n/       731.0   \n",
       "8   http://mashable.com/2013/01/07/car-of-the-futu...       731.0   \n",
       "9   http://mashable.com/2013/01/07/chuck-hagel-web...       731.0   \n",
       "10  http://mashable.com/2013/01/07/cosmic-events-d...       731.0   \n",
       "11   http://mashable.com/2013/01/07/crayon-creatures/       731.0   \n",
       "12      http://mashable.com/2013/01/07/creature-cups/       731.0   \n",
       "13          http://mashable.com/2013/01/07/dad-jokes/       731.0   \n",
       "14  http://mashable.com/2013/01/07/downton-abbey-t...       731.0   \n",
       "\n",
       "     n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0              12.0              219.0          0.663594                1.0   \n",
       "1               9.0              255.0          0.604743                1.0   \n",
       "2               9.0              211.0          0.575130                1.0   \n",
       "3               9.0              531.0          0.503788                1.0   \n",
       "4              13.0             1072.0          0.415646                1.0   \n",
       "5              10.0              370.0          0.559889                1.0   \n",
       "6               8.0              960.0          0.418163                1.0   \n",
       "7              12.0              989.0          0.433574                1.0   \n",
       "8              11.0               97.0          0.670103                1.0   \n",
       "9              10.0              231.0          0.636364                1.0   \n",
       "10              9.0             1248.0          0.490050                1.0   \n",
       "11             10.0              187.0          0.666667                1.0   \n",
       "12              9.0              274.0          0.609195                1.0   \n",
       "13              9.0              285.0          0.744186                1.0   \n",
       "14              8.0              259.0          0.562753                1.0   \n",
       "\n",
       "     n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                    0.815385         4.0              2.0        1.0  ...   \n",
       "1                    0.791946         3.0              1.0        1.0  ...   \n",
       "2                    0.663866         3.0              1.0        1.0  ...   \n",
       "3                    0.665635         9.0              0.0        1.0  ...   \n",
       "4                    0.540890        19.0             19.0       20.0  ...   \n",
       "5                    0.698198         2.0              2.0        0.0  ...   \n",
       "6                    0.549834        21.0             20.0       20.0  ...   \n",
       "7                    0.572108        20.0             20.0       20.0  ...   \n",
       "8                    0.836735         2.0              0.0        0.0  ...   \n",
       "9                    0.797101         4.0              1.0        1.0  ...   \n",
       "10                   0.731638        11.0              0.0        1.0  ...   \n",
       "11                   0.800000         7.0              0.0        1.0  ...   \n",
       "12                   0.707602        18.0              2.0       11.0  ...   \n",
       "13                   0.841530         4.0              2.0        0.0  ...   \n",
       "14                   0.644444        19.0              3.0        9.0  ...   \n",
       "\n",
       "     min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                 0.100000                     0.7               -0.350000   \n",
       "1                 0.033333                     0.7               -0.118750   \n",
       "2                 0.100000                     1.0               -0.466667   \n",
       "3                 0.136364                     0.8               -0.369697   \n",
       "4                 0.033333                     1.0               -0.220192   \n",
       "5                 0.136364                     0.6               -0.195000   \n",
       "6                 0.100000                     1.0               -0.224479   \n",
       "7                 0.100000                     1.0               -0.242778   \n",
       "8                 0.400000                     0.8               -0.125000   \n",
       "9                 0.100000                     0.5               -0.238095   \n",
       "10                0.100000                     1.0               -0.415064   \n",
       "11                0.200000                     0.7               -0.262500   \n",
       "12                0.200000                     0.7               -0.310417   \n",
       "13                0.160000                     1.0               -0.337889   \n",
       "14                0.136364                     0.5               -0.138690   \n",
       "\n",
       "     min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.6000               -0.200000             0.500000   \n",
       "1                  -0.1250               -0.100000             0.000000   \n",
       "2                  -0.8000               -0.133333             0.000000   \n",
       "3                  -0.6000               -0.166667             0.000000   \n",
       "4                  -0.5000               -0.050000             0.454545   \n",
       "5                  -0.4000               -0.100000             0.642857   \n",
       "6                  -0.5000               -0.050000             0.000000   \n",
       "7                  -0.5000               -0.050000             1.000000   \n",
       "8                  -0.1250               -0.125000             0.125000   \n",
       "9                  -0.5000               -0.100000             0.000000   \n",
       "10                 -1.0000               -0.100000             0.000000   \n",
       "11                 -0.4000               -0.125000             0.000000   \n",
       "12                 -0.6000               -0.050000             1.000000   \n",
       "13                 -0.7000               -0.100000             1.000000   \n",
       "14                 -0.1875               -0.050000             0.750000   \n",
       "\n",
       "     title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                   -0.187500                 0.000000   \n",
       "1                    0.000000                 0.500000   \n",
       "2                    0.000000                 0.500000   \n",
       "3                    0.000000                 0.500000   \n",
       "4                    0.136364                 0.045455   \n",
       "5                    0.214286                 0.142857   \n",
       "6                    0.000000                 0.500000   \n",
       "7                    0.500000                 0.500000   \n",
       "8                    0.000000                 0.375000   \n",
       "9                    0.000000                 0.500000   \n",
       "10                   0.000000                 0.500000   \n",
       "11                   0.000000                 0.500000   \n",
       "12                  -1.000000                 0.500000   \n",
       "13                  -1.000000                 0.500000   \n",
       "14                   0.550000                 0.250000   \n",
       "\n",
       "     abs_title_sentiment_polarity   shares  \n",
       "0                        0.187500    593.0  \n",
       "1                        0.000000    711.0  \n",
       "2                        0.000000   1500.0  \n",
       "3                        0.000000   1200.0  \n",
       "4                        0.136364    505.0  \n",
       "5                        0.214286    855.0  \n",
       "6                        0.000000    556.0  \n",
       "7                        0.500000    891.0  \n",
       "8                        0.000000   3600.0  \n",
       "9                        0.000000    710.0  \n",
       "10                       0.000000   2200.0  \n",
       "11                       0.000000   1900.0  \n",
       "12                       1.000000    823.0  \n",
       "13                       1.000000  10000.0  \n",
       "14                       0.550000    761.0  \n",
       "\n",
       "[15 rows x 61 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the easy way, just take the samples from the first one to the 15th\n",
    "online_news_popularity[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Plot the histogram distribution of \"shares\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We plot the histogram distribution of the target \"shares\"\n",
    "# First we compute the histogram\n",
    "plt.hist(online_news_popularity[' shares'])\n",
    "# We give a title to the plot\n",
    "plt.title(\"Histogram distribution of shares\")\n",
    "# We show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) A bar chart counting the attributes: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For this code I use a guide provided by Stack Overflow\n",
    "# Now we write the code for print this bar chart\n",
    "# First we create an array that contains all the attributes (columns for wich we want a bar counting plot)\n",
    "# First we create an array for all the attributes\n",
    "\n",
    "attributes = [\" data_channel_is_lifestyle\", \" data_channel_is_entertainment\", \" data_channel_is_bus\", \" data_channel_is_socmed\", \" data_channel_is_tech\", \" data_channel_is_world\"]\n",
    "# Thus this attributes are boolean in our dataset and so we create two array for 0 and 1\n",
    "\n",
    "tot_0 = []\n",
    "tot_1 = []\n",
    "\n",
    "for attribute in attributes:\n",
    "    tot_0.append(online_news_popularity[attribute].value_counts()[0])\n",
    "    tot_1.append(online_news_popularity[attribute].value_counts()[1])\n",
    "\n",
    "# We declare the figure or the plot (width, height)\n",
    "plt.figure(figsize=[20,10])\n",
    "\n",
    "# We use numpy to group different data with bars\n",
    "X = np.arange(len(tot_0))\n",
    "\n",
    "for attribute in attributes:\n",
    "    plt.bar(X, tot_0, color='black', width=0.25)\n",
    "    plt.bar(X + 0.25, tot_1, color='orange', width=0.25)\n",
    "\n",
    "# We create the legend of the bars in the plot\n",
    "plt.legend(['0','1'])\n",
    "\n",
    "# We overide the x axis \n",
    "plt.xticks([i + 0.25 for i in range(len(attributes))], attributes)\n",
    "\n",
    "# We give a name to our plot\n",
    "plt.title(\"Bar chart counting the attributes\")\n",
    "\n",
    "# We give a name for the axis x and y\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Tot')\n",
    "\n",
    "# We show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments for step 2.2 Dataset Analysis\n",
    "\n",
    "### Comments Step 2.2\n",
    "For this step I used a series of functions to answer these questions, however the above code is not the only solution to answer these questions and so now I will give a brief explanation on the other possible solutions. With the shape method, we can see not only the number of samples, identified by the number (39634) but also the number of features that we have in the DataSet. (Number of features is also the number of columns in the CSV file).\n",
    "With the method lenght we can see only the lenght of the DataFrame and not the number of features in the DataSet.\n",
    "For the second question there are different ways to answer it, in the cell above I reported the simpler version, but there are other methods, like: 1) online_news_popularity.loc[:15] that select the rows where we are interested to \"by label\"; 2) online_news_popularity[:15,a:b] that select the rows where we are interested in \"by position\" and also allow us to select a range of features that we want to show: 'a' and 'b' represent, respectively, the start and the finish column (feature) that we want to show.\\\n",
    "To answer the last question I used a stack overflow guide as specified in the code comments, re-adjusting the code so that it can be made functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature importance analysis  (up to 1 of 11.2 points)\n",
    "\n",
    "Perform feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing phase is fundamental in Machine Learning, in fact, rarely data are ready to use, they can contain typos, they can have missing values and also data could be not normalized. We need so to do some pre-processing work, to \"clean\" and normalize data. The first thing that we do is handle the missing values; there is a different way to handle this:\n",
    "- We can simply discard the entire rows end/or columns that contain the missing value. The problem with this approach is that we can discard useful sample(s);\n",
    "- We can impute the missing value. This approach is better than the first one because we try to \"predict\" the possible value of the missing data. There are different algorithms, like univariate and multivariate feature imputation and finally KNN imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39644, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write here the code for 2.4 Feature importance analysis\n",
    "# We use shape function to find the number of samples of the dataset (this is a mandatory step in section 2.2)\n",
    "online_news_popularity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature with missing value(s):  num_imgs; Number of not missing value:  39643\n",
      "Feature with missing value(s):  num_videos; Number of not missing value:  39642\n",
      "Feature with missing value(s):  weekday_is_sunday; Number of not missing value:  39643\n",
      "Feature with missing value(s):  shares; Number of not missing value:  39643\n"
     ]
    }
   ],
   "source": [
    "# We know that the total samples are 39,644, and so we print all the feature(s) \n",
    "# that have a total number of value less then 39,644 (i.e. that have at least one missing value)\n",
    "for el in online_news_popularity:\n",
    "    tot = online_news_popularity[el].count()\n",
    "    if tot < 39644:\n",
    "        print(\"Feature with missing value(s):\",el+\";\" , \"Number of not missing value: \", tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the code above many features in the DataSet have missing values. In general, when the missing values are <= 10% of the total number of samples these are discarded, instead if the missing values are >= 50% of the total number of samples, then all the features with the missing values are discarded (let's drop of the column). In the event that the number of missing values is around 22% of the total samples, we are going to impute these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have that the missing values are <= 10% of the total number of samples and then we can discard them. No we show the code to discard (we drop them) this rows of the dataset, we'll use dropna function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>http://mashable.com/2014/12/27/samsung-app-aut...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>http://mashable.com/2014/12/27/seth-rogen-jame...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>http://mashable.com/2014/12/27/son-pays-off-mo...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>http://mashable.com/2014/12/27/ukraine-blasts/</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>http://mashable.com/2014/12/27/youtube-channel...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39639 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url   timedelta  \\\n",
       "0      http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1      http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2      http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3      http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4       http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "...                                                  ...         ...   \n",
       "39639  http://mashable.com/2014/12/27/samsung-app-aut...         8.0   \n",
       "39640  http://mashable.com/2014/12/27/seth-rogen-jame...         8.0   \n",
       "39641  http://mashable.com/2014/12/27/son-pays-off-mo...         8.0   \n",
       "39642     http://mashable.com/2014/12/27/ukraine-blasts/         8.0   \n",
       "39643  http://mashable.com/2014/12/27/youtube-channel...         8.0   \n",
       "\n",
       "        n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0                 12.0              219.0          0.663594   \n",
       "1                  9.0              255.0          0.604743   \n",
       "2                  9.0              211.0          0.575130   \n",
       "3                  9.0              531.0          0.503788   \n",
       "4                 13.0             1072.0          0.415646   \n",
       "...                ...                ...               ...   \n",
       "39639             11.0              346.0          0.529052   \n",
       "39640             12.0              328.0          0.696296   \n",
       "39641             10.0              442.0          0.516355   \n",
       "39642              6.0              682.0          0.539493   \n",
       "39643             10.0              157.0          0.701987   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens   num_hrefs  \\\n",
       "0                    1.0                   0.815385         4.0   \n",
       "1                    1.0                   0.791946         3.0   \n",
       "2                    1.0                   0.663866         3.0   \n",
       "3                    1.0                   0.665635         9.0   \n",
       "4                    1.0                   0.540890        19.0   \n",
       "...                  ...                        ...         ...   \n",
       "39639                1.0                   0.684783         9.0   \n",
       "39640                1.0                   0.885057         9.0   \n",
       "39641                1.0                   0.644128        24.0   \n",
       "39642                1.0                   0.692661        10.0   \n",
       "39643                1.0                   0.846154         1.0   \n",
       "\n",
       "        num_self_hrefs   num_imgs  ...   min_positive_polarity  \\\n",
       "0                  2.0        1.0  ...                0.100000   \n",
       "1                  1.0        1.0  ...                0.033333   \n",
       "2                  1.0        1.0  ...                0.100000   \n",
       "3                  0.0        1.0  ...                0.136364   \n",
       "4                 19.0       20.0  ...                0.033333   \n",
       "...                ...        ...  ...                     ...   \n",
       "39639              7.0        1.0  ...                0.100000   \n",
       "39640              7.0        3.0  ...                0.136364   \n",
       "39641              1.0       12.0  ...                0.136364   \n",
       "39642              1.0        1.0  ...                0.062500   \n",
       "39643              1.0        0.0  ...                0.100000   \n",
       "\n",
       "        max_positive_polarity   avg_negative_polarity   min_negative_polarity  \\\n",
       "0                        0.70               -0.350000                  -0.600   \n",
       "1                        0.70               -0.118750                  -0.125   \n",
       "2                        1.00               -0.466667                  -0.800   \n",
       "3                        0.80               -0.369697                  -0.600   \n",
       "4                        1.00               -0.220192                  -0.500   \n",
       "...                       ...                     ...                     ...   \n",
       "39639                    0.75               -0.260000                  -0.500   \n",
       "39640                    0.70               -0.211111                  -0.400   \n",
       "39641                    0.50               -0.356439                  -0.800   \n",
       "39642                    0.50               -0.205246                  -0.500   \n",
       "39643                    0.50               -0.200000                  -0.200   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500    593.0  \n",
       "1                     0.500000                       0.000000    711.0  \n",
       "2                     0.500000                       0.000000   1500.0  \n",
       "3                     0.500000                       0.000000   1200.0  \n",
       "4                     0.045455                       0.136364    505.0  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000   1800.0  \n",
       "39640                 0.200000                       1.000000   1900.0  \n",
       "39641                 0.045455                       0.136364   1900.0  \n",
       "39642                 0.500000                       0.000000   1100.0  \n",
       "39643                 0.166667                       0.250000   1300.0  \n",
       "\n",
       "[39639 rows x 61 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing that the missing values of all the features are less than 10% we can remove the sample rows\n",
    "# We use the dropna function of pandas that allows to delete missing values\n",
    "online_news_popularity = online_news_popularity.dropna(how='any')\n",
    "online_news_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to check if there are still missing values in our dataset that were not previously considered, using the isnull function. We show the code to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                              0\n",
       " timedelta                       0\n",
       " n_tokens_title                  0\n",
       " n_tokens_content                0\n",
       " n_unique_tokens                 0\n",
       "                                ..\n",
       " title_subjectivity              0\n",
       " title_sentiment_polarity        0\n",
       " abs_title_subjectivity          0\n",
       " abs_title_sentiment_polarity    0\n",
       " shares                          0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the isnull() and sum() functions to count the number of missing values\n",
    "online_news_popularity.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of url are unique, i.e. if <b>n</b> is the number of samples then there are <b>n</b> different values of url. For this reason it is better to drop this feature (column) insted of applying an encoding method (e.g. one-hot encoding) to transform into numerical values. Now we show the code to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39639 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0           731.0             12.0              219.0          0.663594   \n",
       "1           731.0              9.0              255.0          0.604743   \n",
       "2           731.0              9.0              211.0          0.575130   \n",
       "3           731.0              9.0              531.0          0.503788   \n",
       "4           731.0             13.0             1072.0          0.415646   \n",
       "...           ...              ...                ...               ...   \n",
       "39639         8.0             11.0              346.0          0.529052   \n",
       "39640         8.0             12.0              328.0          0.696296   \n",
       "39641         8.0             10.0              442.0          0.516355   \n",
       "39642         8.0              6.0              682.0          0.539493   \n",
       "39643         8.0             10.0              157.0          0.701987   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens   num_hrefs  \\\n",
       "0                    1.0                   0.815385         4.0   \n",
       "1                    1.0                   0.791946         3.0   \n",
       "2                    1.0                   0.663866         3.0   \n",
       "3                    1.0                   0.665635         9.0   \n",
       "4                    1.0                   0.540890        19.0   \n",
       "...                  ...                        ...         ...   \n",
       "39639                1.0                   0.684783         9.0   \n",
       "39640                1.0                   0.885057         9.0   \n",
       "39641                1.0                   0.644128        24.0   \n",
       "39642                1.0                   0.692661        10.0   \n",
       "39643                1.0                   0.846154         1.0   \n",
       "\n",
       "        num_self_hrefs   num_imgs   num_videos  ...   min_positive_polarity  \\\n",
       "0                  2.0        1.0          0.0  ...                0.100000   \n",
       "1                  1.0        1.0          0.0  ...                0.033333   \n",
       "2                  1.0        1.0          0.0  ...                0.100000   \n",
       "3                  0.0        1.0          0.0  ...                0.136364   \n",
       "4                 19.0       20.0          0.0  ...                0.033333   \n",
       "...                ...        ...          ...  ...                     ...   \n",
       "39639              7.0        1.0          1.0  ...                0.100000   \n",
       "39640              7.0        3.0         48.0  ...                0.136364   \n",
       "39641              1.0       12.0          1.0  ...                0.136364   \n",
       "39642              1.0        1.0          0.0  ...                0.062500   \n",
       "39643              1.0        0.0          2.0  ...                0.100000   \n",
       "\n",
       "        max_positive_polarity   avg_negative_polarity   min_negative_polarity  \\\n",
       "0                        0.70               -0.350000                  -0.600   \n",
       "1                        0.70               -0.118750                  -0.125   \n",
       "2                        1.00               -0.466667                  -0.800   \n",
       "3                        0.80               -0.369697                  -0.600   \n",
       "4                        1.00               -0.220192                  -0.500   \n",
       "...                       ...                     ...                     ...   \n",
       "39639                    0.75               -0.260000                  -0.500   \n",
       "39640                    0.70               -0.211111                  -0.400   \n",
       "39641                    0.50               -0.356439                  -0.800   \n",
       "39642                    0.50               -0.205246                  -0.500   \n",
       "39643                    0.50               -0.200000                  -0.200   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500    593.0  \n",
       "1                     0.500000                       0.000000    711.0  \n",
       "2                     0.500000                       0.000000   1500.0  \n",
       "3                     0.500000                       0.000000   1200.0  \n",
       "4                     0.045455                       0.136364    505.0  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000   1800.0  \n",
       "39640                 0.200000                       1.000000   1900.0  \n",
       "39641                 0.045455                       0.136364   1900.0  \n",
       "39642                 0.500000                       0.000000   1100.0  \n",
       "39643                 0.166667                       0.250000   1300.0  \n",
       "\n",
       "[39639 rows x 60 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the drop function of pandas dataframe where we specify the column (feature) to drop and the axis (1 for column, 0 for row)\n",
    "online_news_popularity = online_news_popularity.drop(['url'], axis=1)\n",
    "online_news_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The answer to this question and the comments are given in the Evaluation section of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments for step 2.4 Feature importance analysis\n",
    "\n",
    "### Comments 2.4\n",
    "For this step I analyzed the missing values present in the dataset highlighting that they were less than 10% of the total number of samples and therefore I decided to delete these lines. I deleted the url column as it is useless for the purposes of this challenge. The final part of this step is carried out in the Model selection section in order to highlight which features have the greatest impact during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Model Selection (up to 8.2 of 11.2  points)\n",
    "In this part of the challenge you are requested to perform all the necessary steps required in order to design a full fledged classification task on the <b>shares</b> feature.\n",
    "\n",
    "You are requested to perform the following steps having in mind the following: \n",
    "\n",
    "1) the dataset must be properly splitted to perform crossvalidation \n",
    "\n",
    "2) when required, features must be properly encoded\n",
    "\n",
    "3) in order to simplify the problem the target feature can be dicretized <b>(number of classes must be >=5)</b> ;\n",
    "\n",
    "4) for model selection you are requested to consider: \n",
    "\n",
    "- Decision Trees\n",
    "\n",
    "- Support Vector Machines;\n",
    "\n",
    "- An ensamble methodology;\n",
    "\n",
    "- MLPNs.\n",
    "\n",
    "5) hyper-parameter tuning <b>must</b> be performed and discussed;\n",
    "\n",
    "6) apply standardizion and normalization when appropriate;\n",
    "\n",
    "7) remember to use an appropriate evaluation setting (cross-fold etc..)\n",
    "\n",
    "8) describe the measures adopted for the evaluation and discuss the results;\n",
    "\n",
    "9) provide a discussion of the model selection, where you describe the differences in terms of performance and explains the root causes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the code for step 3 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now without missing value, we can so now continue the pre-processing phase. We discretize the class to predict, the feature shares, to simplify the problem. I decided to discretize the class to predict with KBinsDiscretizer that discretize feature into k bins, in this case we use 5 bins. In the following, we show the code to compute this operation. In the paper, two bins are used to discretize the target, for this reason I decided to use the minimum required for delivery (i.e. 5 bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0       731.0             12.0              219.0          0.663594   \n",
       "1       731.0              9.0              255.0          0.604743   \n",
       "2       731.0              9.0              211.0          0.575130   \n",
       "3       731.0              9.0              531.0          0.503788   \n",
       "4       731.0             13.0             1072.0          0.415646   \n",
       "\n",
       "    n_non_stop_words   n_non_stop_unique_tokens   num_hrefs   num_self_hrefs  \\\n",
       "0                1.0                   0.815385         4.0              2.0   \n",
       "1                1.0                   0.791946         3.0              1.0   \n",
       "2                1.0                   0.663866         3.0              1.0   \n",
       "3                1.0                   0.665635         9.0              0.0   \n",
       "4                1.0                   0.540890        19.0             19.0   \n",
       "\n",
       "    num_imgs   num_videos  ...   min_positive_polarity  \\\n",
       "0        1.0          0.0  ...                0.100000   \n",
       "1        1.0          0.0  ...                0.033333   \n",
       "2        1.0          0.0  ...                0.100000   \n",
       "3        1.0          0.0  ...                0.136364   \n",
       "4       20.0          0.0  ...                0.033333   \n",
       "\n",
       "    max_positive_polarity   avg_negative_polarity   min_negative_polarity  \\\n",
       "0                     0.7               -0.350000                  -0.600   \n",
       "1                     0.7               -0.118750                  -0.125   \n",
       "2                     1.0               -0.466667                  -0.800   \n",
       "3                     0.8               -0.369697                  -0.600   \n",
       "4                     1.0               -0.220192                  -0.500   \n",
       "\n",
       "    max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0               -0.200000             0.500000                  -0.187500   \n",
       "1               -0.100000             0.000000                   0.000000   \n",
       "2               -0.133333             0.000000                   0.000000   \n",
       "3               -0.166667             0.000000                   0.000000   \n",
       "4               -0.050000             0.454545                   0.136364   \n",
       "\n",
       "    abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                 0.000000                       0.187500      0.0  \n",
       "1                 0.500000                       0.000000      0.0  \n",
       "2                 0.500000                       0.000000      2.0  \n",
       "3                 0.500000                       0.000000      2.0  \n",
       "4                 0.045455                       0.136364      0.0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To simplify the problem we discretize our target ' shares' by using 5 bins\n",
    "# We show the code to do this, first we create the KBinsDiscretizer and then we apply fit_transform to the target\n",
    "kbin = preprocessing.KBinsDiscretizer(n_bins=5, encode='ordinal')\n",
    "online_news_popularity[' shares'] = kbin.fit_transform(online_news_popularity[[' shares']])\n",
    "\n",
    "#with .head() we can see the first 5 rows of the dataset \n",
    "online_news_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with our preprocessing work and we do the class balancement. This operation is really important because many algorithms suffer if the classes are not balanced, if we have two classes, call it class A and class B if class A represents 90% of the dataset an algorithm can completely ignore the minority class (class B). In the next subsections we will use the Decision Tree and SVM classifier and if data are not balanced the performance evaluation won't be the best ones. So we have to balance the class to avoid this problem and we can do it with:\n",
    "\n",
    "Down-sample the majority class, but this solution can discard useful sample of the dataset;\n",
    "Up-sample the minority class, but this can cause overfitting and obtain a non-general model;\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique), that creates new instances of the minority class by forming a convex combination of the neighboring instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    8823\n",
       "4.0    8079\n",
       "3.0    8009\n",
       "0.0    7926\n",
       "1.0    6802\n",
       "Name:  shares, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For each different value in the class to predict (there are 5 different values), we print the total number\n",
    "#of occurences of that value\n",
    "online_news_popularity[' shares'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in this case, the value 2.0 has a major number of samples, instead, the value 1.0 has a minor number of samples. We are going to call so the samples with a class value equal to 2.0 the majority class, and all the other samples with different class values as the minority classes (and so we have four minority classes and one majority class). To balance the classes we apply the SMOTE technique. We are going to show now the code to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    8823\n",
       "2.0    8823\n",
       "1.0    8823\n",
       "4.0    8823\n",
       "3.0    8823\n",
       "Name:  shares, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use a copy od the original dataframe and we define as X the dataframe that do not contain the target class\n",
    "online_news_popularity_upsampled = online_news_popularity.copy()\n",
    "X = online_news_popularity_upsampled.loc[:, online_news_popularity.columns != ' shares']\n",
    "\n",
    "#We define as y the dataframe that contain only the target class\n",
    "y = online_news_popularity_upsampled[' shares']\n",
    "\n",
    "#We apply now the SMOTE technique\n",
    "smote_enc = SMOTE(random_state=0, sampling_strategy='auto', k_neighbors=5)\n",
    "X_res, y_res = smote_enc.fit_resample(X, y)\n",
    "\n",
    "#We unify now X resampled and y resemled\n",
    "online_news_popularity_upsampled = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "\n",
    "#We now show how many instaces we have for each classes\n",
    "online_news_popularity_upsampled[' shares'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, classess are now balanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the pre-processing phase, we can scale and normalize the dataset. Many algorithms in Machine Learning obtain better performance evaluation if data have a normal (Gaussian) distribution and data are normalized. One of these is the Multi Layer Perceptron Networks, a model that we will select in the next section; also SVM needs normalized data, even if it is scaling invariant. So is important to normalize and scale data to obtain a better performance evaluation. We will use also the normalized and scaled data for the other selected models, this operation on the data does not impact other models' performance evaluation.\n",
    "<ol>\n",
    "    <li>\n",
    "        Scaling:</br>\n",
    "        There is differnt way to scale data, like:\n",
    "        <ul>\n",
    "            <li>Standard scaling</li>\n",
    "            <li>Min/Max scaling</li>\n",
    "            <li>Max Abs scaling</li>\n",
    "            <li>Mapping to a Uniform distribution</li>\n",
    "            <li>Mapping to a Gaussian distribution</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Normalization:\n",
    "        The process of normalization is used to have unit norm. Function <i>normalize</i> in sklearn provides a quick and easy way to perform this operation; normalize function allow us to do:\n",
    "        <ul>\n",
    "            <li><i>l1 norm</i></li>\n",
    "            <li><i>l2 norm</i></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "We decided to map our numerical features into a Gaussian distribution and to normalize them with <i>l2 norm</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we show the code to scale numerical features into a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of feature title_sentiment_polarity before the scaling\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb2klEQVR4nO3de5xddX3u8c9DAhEvIJKRQi4EJaKBaoQ0prUoNrYEvAQ92AatQUsb4chRT+2pYC9S21Q41eLJywIFoVyqXASUqOCRSgGtXBw0QgKiwzVDIgS5BZBowtM/1m90Mdkzs2fvPTOZyfN+vfZr1v6u2++3Z7KftX5r7R3ZJiIiYoexbkBERGwbEggREQEkECIiokggREQEkECIiIgigRAREUACYcKTNFPSk5ImDbKMJe07mu0q+z1EUu8g88+Q9DcjsN+mttvMa7ctkfQOSWtLm1871u0Zjv5/C5LWSDpk7Fq0fUogTDCS7pX05r7ntu+3/ULbW8r8ayX96di1sHm2j7X99+1sQ9L7JH2nle2O9mtXtv9MeUN/XNL1kn5zGJv4NHB8afMPRqqdo8H2/ravHet2bG8SCBHbluNtvxDYHbgWuGAY6+4NrGllp+PlLChGVgJhApF0ATAT+Go5yvxLSbPKkNBkScuBg4HPlfmfa7CNKZI+Lel+SQ+W4ZWdh9jvVElfk/SYpEckfVvSDmXec4ajJJ0r6R/6rf9xSQ+Xs5v3DLSspLdKWlX2811Jr67NmyHpckkbJP1M0uckvQo4A/jt0t/H+m9X0h2S3lrbzuTSlgOHeu0k/Yukz/Try1clfWSw16sZtjcDFwFzatveQdIJku4qfbxE0kvK7+xJYBLwQ0l3leVfVc46HitDMG/v99qeLulKSU8Bb5K0l6TLymt4j6QPDdQ+SYdLul3SRkkPSPqL2rzF5ff0RGnrolJ/f3m9N0q6W9IHBtn+r850JZ1U+np+WXeNpHm1ZQ+U9IMy70uSLu7/NxZNsp3HBHoA9wJvrj2fBRiYXJ5fC/xpv3UM7FumPwusBF4CvAj4KvCpIfb5Kao33h3L42BA/bddnp8L/EOZPgTYDPwzMAV4I/AUsF+DZQ8EHgJeR/XGd3Tp65Ty/IfAqcALgOcBv1vWex/wnX7trW/3b4Ev1Oa9BfhRM68dMB9YB+xQnk8Fngb2aPF396vtAzsBy4Hra/M/AtwITC/9/lfgwgF+jzsCPcDHy7Z+D9jY77V9HHg91YHh84FbyuuxE/Ay4G7g0AHauh44uEzvBhxYe00eB36/bHca8Mraa/tyQOV3/XRtvUOA3kZ/x8BJwDPA4eV3/SngxtrrdB/w4dLndwK/6Pv95jG8R84Q4lckCfgz4H/bfsT2RuAfgSVDrPpLYE9gb9u/tP1tl3+tTfob25tsXwd8HfjDBsv8GfCvtm+yvcX2ecAmYAHVm9BewP+x/ZTtZ2x/p8E2Gvki8HZJzy/P311qQ7J9M9Wb38JSWgJca/vBJvfdyIpyJvMkcDzwd7V5HwD+ynav7U1Ub5RHSprcYDsLgBcCJ9v+he1rgK8BR9WWucL2f9l+FvhNoMv2J8vydwNnMfDv/pfAHEm72H7U9vdL/RjgHNtX237W9gO2fwRg++u273LlOuCbVAcPzfiO7StdXc+5AHhNrZ+TgRXlb+9y4OYmtxn9JBCirotypFiGGR4DvlHqg/knqqPRb5ahgBOGsc9HbT9Ve34f1Zt7f3sDH+1rV2nbjLLsDOA+V8Msw2K7B7gDeFsJhbfTZCAU5wF/XKb/mAHG/Muw2JPlccYg2/uQ7RdTneW8Fbi0NjS2N/DlWv/vALYAezTYzl7A2vJm3+c+qiP2Pmtr03sDe/V7fT8+wLYB/gfVEft9kq6T9NulPgO4q9EKkg6TdKOqYcXHyvpTB9h+fz+tTT8NPK8E4V7AA/0OQNYSLWl0ZBHj21BH5oPNfxj4ObC/7Qea3mF1JvFRqjfs/YH/lPQ929+i+sf7/NrivwHUbzXdTdILaqEwE1jdYDdrgeW2l/efUd6MZkqa3CAUmjlTuZDqyHkH4PYSEo002ta/A6slvQZ4FfCVhiva/0h1ttWU8kb+bUk9wB8At1K9Bn9i+7+a2MQ6YIakHWqhMBP4cX03tem1wD22ZzfZvu8BiyXtSHUmcwlVGKylGhZ6DklTgMuApVRnJr+U9BWq4aN2rAemSVItFAYMpRhczhAmngepxn+HPb+8cZwFnCrppQCSpkk6dLAdqrrYu28ZcnqC6qh1S5m9Cni3pEnl4uIbG2zi7yTtJOlgqqPiLzVY5izgWEmvU+UFkt4i6UVUQwTrgZNL/XmSXl/r73RJOw3ShYuo3nSPY/Czg61eO9u9wPeozgwus/3zQdYflhJ0c/j1nUNnAMsl7V3md0laPMDqN1Fdj/lLSTuquqf/bVR9beRm4AlJH5O0c/l9HSDptxq0aydJ75G0q+1f8uvfOcDZwPslLVR1EXyapFdSjfVPATYAmyUdRvWat+uGsu/jVV38X0w1hBgtSCBMPJ8C/rqc9v9Fg/n/j2rc+VFJKxrM/xjV8M+Nkp4A/gPYb4h9zi7LPUn1D/Q0//oe8g9TvRE9BryHrY+gfwo8SnVE+wXg2L4x5zrb3VTXET5Xlu+humBMGVd+G7AvcD/VGcgflVWvoXpD/amkhxs13vb60u7fAS4epJ8DvXbnUY3BD+cW0YH03cX0ZNneX9u+qrb/lVRDcxupLjC/rtFGbP+CavjrMKozv9OApY1e27J832s4F7inrPN5YNcB2vle4N7yN3IsZdisXFd5P9UF/seB66iuLW0EPkR1JvEo1bWalU28HoMq/Xwn1bWLx0o7vkZ1fSmGSR7Wtb+I0SPpfKDH9ifHui2DkfQGqqGjWf3G7GMMSLoJOMP2v411W8abnCHENqlcMNyP6mh1m1XG0D8MfD5hMDYkvVHSb5Qho6OBV1PdDBHDlECIpvS7S6b+uGrotVvyU6ohgMtGaPttU/XBt8eobrn97Jg2Zvu2H9XnUB6nurnhyDIMGMOUIaOIiAByhhAREcW4/RzC1KlTPWvWrLFuRkTEuHLLLbc8bLvhh03HbSDMmjWL7u7usW5GRMS4Ium+geZlyCgiIoAEQkREFAmEiIgAEggREVEkECIiAkggREREkUCIiAgggRAREUUCISIigHH8SeV2zDrh62O273tPfsuY7TsiYjA5Q4iICCCBEBERRQIhIiKABEJERBQJhIiIABIIERFRJBAiIgJIIERERJFAiIgIoIlAkHSOpIckra7VLpa0qjzulbSq1GdJ+nlt3hm1dQ6SdJukHkkrJKnUp5Tt9Ui6SdKsznczIiKG0swZwrnAonrB9h/Znmt7LnAZcHlt9l1982wfW6ufDiwDZpdH3zaPAR61vS9wKnBKKx2JiIj2DBkItq8HHmk0rxzl/yFw4WDbkLQnsIvtG2wbOB84osxeDJxXpi8FFvadPURExOhp9xrCwcCDtn9Sq+0j6QeSrpN0cKlNA3pry/SWWt+8tQC2NwOPA7u32a6IiBimdr/t9Ciee3awHphp+2eSDgK+Iml/oNERv8vPweY9h6RlVMNOzJw5s+VGR0TE1lo+Q5A0GXgncHFfzfYm2z8r07cAdwGvoDojmF5bfTqwrkz3AjNq29yVAYaobJ9pe57teV1dXa02PSIiGmhnyOjNwI9s/2ooSFKXpEll+mVUF4/vtr0e2ChpQbk+sBS4oqy2Eji6TB8JXFOuM0RExChq5rbTC4EbgP0k9Uo6psxawtYXk98A3Crph1QXiI+13Xe0fxzweaCH6szhqlI/G9hdUg/w58AJbfQnIiJaNOQ1BNtHDVB/X4PaZVS3oTZavhs4oEH9GeBdQ7UjIiJGVj6pHBERQAIhIiKKBEJERAAJhIiIKBIIEREBJBAiIqJIIEREBJBAiIiIIoEQERFAAiEiIooEQkREAAmEiIgoEggREQEkECIiokggREQEkECIiIgigRAREUACISIiigRCREQATQSCpHMkPSRpda12kqQHJK0qj8Nr806U1CPpTkmH1uoHSbqtzFshSaU+RdLFpX6TpFkd7mNERDShmTOEc4FFDeqn2p5bHlcCSJoDLAH2L+ucJmlSWf50YBkwuzz6tnkM8KjtfYFTgVNa7EtERLRhyECwfT3wSJPbWwxcZHuT7XuAHmC+pD2BXWzfYNvA+cARtXXOK9OXAgv7zh4iImL0tHMN4XhJt5Yhpd1KbRqwtrZMb6lNK9P9689Zx/Zm4HFg90Y7lLRMUrek7g0bNrTR9IiI6K/VQDgdeDkwF1gPfKbUGx3Ze5D6YOtsXbTPtD3P9ryurq5hNTgiIgbXUiDYftD2FtvPAmcB88usXmBGbdHpwLpSn96g/px1JE0GdqX5IaqIiOiQlgKhXBPo8w6g7w6klcCScufQPlQXj2+2vR7YKGlBuT6wFLiits7RZfpI4JpynSEiIkbR5KEWkHQhcAgwVVIv8AngEElzqYZ27gU+AGB7jaRLgNuBzcAHbW8pmzqO6o6lnYGrygPgbOACST1UZwZLOtCviIgYpiEDwfZRDcpnD7L8cmB5g3o3cECD+jPAu4ZqR0REjKx8UjkiIoAEQkREFAmEiIgAEggREVEkECIiAkggREREkUCIiAgggRAREUUCISIigARCREQUCYSIiAASCBERUSQQIiICSCBERESRQIiICCCBEBERRQIhIiKABEJERBQJhIiIAJoIBEnnSHpI0upa7Z8k/UjSrZK+LOnFpT5L0s8lrSqPM2rrHCTpNkk9klZIUqlPkXRxqd8kaVbnuxkREUNp5gzhXGBRv9rVwAG2Xw38GDixNu8u23PL49ha/XRgGTC7PPq2eQzwqO19gVOBU4bdi4iIaNuQgWD7euCRfrVv2t5cnt4ITB9sG5L2BHaxfYNtA+cDR5TZi4HzyvSlwMK+s4eIiBg9nbiG8CfAVbXn+0j6gaTrJB1catOA3toyvaXWN28tQAmZx4HdG+1I0jJJ3ZK6N2zY0IGmR0REn7YCQdJfAZuBL5TSemCm7dcCfw58UdIuQKMjfvdtZpB5zy3aZ9qeZ3teV1dXO02PiIh+Jre6oqSjgbcCC8swELY3AZvK9C2S7gJeQXVGUB9Wmg6sK9O9wAygV9JkYFf6DVFFRMTIa+kMQdIi4GPA220/Xat3SZpUpl9GdfH4btvrgY2SFpTrA0uBK8pqK4Gjy/SRwDV9ARMREaNnyDMESRcChwBTJfUCn6C6q2gKcHW5/ntjuaPoDcAnJW0GtgDH2u472j+O6o6lnamuOfRddzgbuEBSD9WZwZKO9CwiIoZlyECwfVSD8tkDLHsZcNkA87qBAxrUnwHeNVQ7IiJiZOWTyhERASQQIiKiSCBERASQQIiIiCKBEBERQAIhIiKKBEJERAAJhIiIKBIIEREBJBAiIqJIIEREBJBAiIiIIoEQERFAAiEiIooEQkREAAmEiIgoEggREQEkECIiohgyECSdI+khSatrtZdIulrST8rP3WrzTpTUI+lOSYfW6gdJuq3MW6HynzFLmiLp4lK/SdKsDvcxIiKa0MwZwrnAon61E4Bv2Z4NfKs8R9IcYAmwf1nnNEmTyjqnA8uA2eXRt81jgEdt7wucCpzSamciIqJ1QwaC7euBR/qVFwPnlenzgCNq9Ytsb7J9D9ADzJe0J7CL7RtsGzi/3zp927oUWNh39hAREaOn1WsIe9heD1B+vrTUpwFra8v1ltq0Mt2//px1bG8GHgd2b7RTScskdUvq3rBhQ4tNj4iIRjp9UbnRkb0HqQ+2ztZF+0zb82zP6+rqarGJERHRSKuB8GAZBqL8fKjUe4EZteWmA+tKfXqD+nPWkTQZ2JWth6giImKEtRoIK4Gjy/TRwBW1+pJy59A+VBePby7DShslLSjXB5b2W6dvW0cC15TrDBERMYomD7WApAuBQ4CpknqBTwAnA5dIOga4H3gXgO01ki4Bbgc2Ax+0vaVs6jiqO5Z2Bq4qD4CzgQsk9VCdGSzpSM8iImJYhgwE20cNMGvhAMsvB5Y3qHcDBzSoP0MJlIiIGDv5pHJERAAJhIiIKBIIEREBJBAiIqJIIEREBJBAiIiIIoEQERFAAiEiIooEQkREAAmEiIgoEggREQEkECIiokggREQEkECIiIgigRAREUACISIiigRCREQACYSIiCgSCBERAbQRCJL2k7Sq9nhC0kcknSTpgVr98No6J0rqkXSnpENr9YMk3VbmrZCkdjsWERHD03Ig2L7T9lzbc4GDgKeBL5fZp/bNs30lgKQ5wBJgf2ARcJqkSWX504FlwOzyWNRquyIiojWdGjJaCNxl+75BllkMXGR7k+17gB5gvqQ9gV1s32DbwPnAER1qV0RENKlTgbAEuLD2/HhJt0o6R9JupTYNWFtbprfUppXp/vWtSFomqVtS94YNGzrU9IiIgA4EgqSdgLcDXyql04GXA3OB9cBn+hZtsLoHqW9dtM+0Pc/2vK6urnaaHRER/XTiDOEw4Pu2HwSw/aDtLbafBc4C5pfleoEZtfWmA+tKfXqDekREjKJOBMJR1IaLyjWBPu8AVpfplcASSVMk7UN18fhm2+uBjZIWlLuLlgJXdKBdERExDJPbWVnS84HfBz5QK/9fSXOphn3u7Ztne42kS4Dbgc3AB21vKescB5wL7AxcVR4RETGK2goE208Du/ervXeQ5ZcDyxvUu4ED2mlLRES0J59UjogIIIEQERFFAiEiIoAEQkREFAmEiIgAEggREVEkECIiAkggREREkUCIiAgggRAREUUCISIigARCREQUCYSIiAASCBERUSQQIiICSCBERESRQIiICCCBEBERRVuBIOleSbdJWiWpu9ReIulqST8pP3erLX+ipB5Jd0o6tFY/qGynR9IKSWqnXRERMXydOEN4k+25tueV5ycA37I9G/hWeY6kOcASYH9gEXCapEllndOBZcDs8ljUgXZFRMQwjMSQ0WLgvDJ9HnBErX6R7U227wF6gPmS9gR2sX2DbQPn19aJiIhR0m4gGPimpFskLSu1PWyvByg/X1rq04C1tXV7S21ame5fj4iIUTS5zfVfb3udpJcCV0v60SDLNrou4EHqW2+gCp1lADNnzhxuWyMiYhBtnSHYXld+PgR8GZgPPFiGgSg/HyqL9wIzaqtPB9aV+vQG9Ub7O9P2PNvzurq62ml6RET003IgSHqBpBf1TQN/AKwGVgJHl8WOBq4o0yuBJZKmSNqH6uLxzWVYaaOkBeXuoqW1dSIiYpS0M2S0B/DlcofoZOCLtr8h6XvAJZKOAe4H3gVge42kS4Dbgc3AB21vKds6DjgX2Bm4qjwiImIUtRwItu8GXtOg/jNg4QDrLAeWN6h3Awe02paIiGhfPqkcERFA+3cZRUQ/s074+pjt+96T3zJm+47xL2cIEREBJBAiIqJIIEREBJBAiIiIIoEQERFAAiEiIooEQkREAAmEiIgo8sG0mLDG8gNiEeNRzhAiIgJIIERERJEho4iIFkzE76zKGUJERAAJhIiIKBIIEREBJBAiIqLIReXtxES8ABYRndXyGYKkGZL+U9IdktZI+nCpnyTpAUmryuPw2jonSuqRdKekQ2v1gyTdVuatkKT2uhUREcPVzhnCZuCjtr8v6UXALZKuLvNOtf3p+sKS5gBLgP2BvYD/kPQK21uA04FlwI3AlcAi4Ko22hYREcPU8hmC7fW2v1+mNwJ3ANMGWWUxcJHtTbbvAXqA+ZL2BHaxfYNtA+cDR7TaroiIaE1HLipLmgW8FriplI6XdKukcyTtVmrTgLW11XpLbVqZ7l9vtJ9lkroldW/YsKETTY+IiKLtQJD0QuAy4CO2n6Aa/nk5MBdYD3ymb9EGq3uQ+tZF+0zb82zP6+rqarfpERFR01YgSNqRKgy+YPtyANsP2t5i+1ngLGB+WbwXmFFbfTqwrtSnN6hHRMQoaucuIwFnA3fY/udafc/aYu8AVpfplcASSVMk7QPMBm62vR7YKGlB2eZS4IpW2xUREa1p5y6j1wPvBW6TtKrUPg4cJWku1bDPvcAHAGyvkXQJcDvVHUofLHcYARwHnAvsTHV3Ue4wiogYZS0Hgu3v0Hj8/8pB1lkOLG9Q7wYOaLUtERHRvnx1RUREAPnqiogJZay+oiRfTzIxJBBixOX/No4YHzJkFBERQAIhIiKKBEJERAC5hjDqMp4eEduqnCFERASQQIiIiCKBEBERQAIhIiKKBEJERAAJhIiIKHLbaUS0bSxvp873KHVOAiEixrV8tqdzMmQUERFAAiEiIooEQkREAAmEiIgotplAkLRI0p2SeiSdMNbtiYjY3mwTgSBpEvAvwGHAHOAoSXPGtlUREduXbSIQgPlAj+27bf8CuAhYPMZtiojYrmwrn0OYBqytPe8FXtd/IUnLgGXl6ZOS7mxxf1OBh1tcd7xKn7cP6fN2QKe01ee9B5qxrQSCGtS8VcE+Eziz7Z1J3bbntbud8SR93j6kz9uHkerztjJk1AvMqD2fDqwbo7ZERGyXtpVA+B4wW9I+knYClgArx7hNERHblW1iyMj2ZknHA/8fmAScY3vNCO6y7WGncSh93j6kz9uHEemz7K2G6iMiYju0rQwZRUTEGEsgREQEMMEDYaivw1BlRZl/q6QDx6KdndREn99T+nqrpO9Kes1YtLOTmv3aE0m/JWmLpCNHs30joZk+SzpE0ipJayRdN9pt7KQm/q53lfRVST8s/X3/WLSzkySdI+khSasHmN/59y/bE/JBdXH6LuBlwE7AD4E5/ZY5HLiK6nMQC4Cbxrrdo9Dn3wF2K9OHbQ99ri13DXAlcORYt3sUfs8vBm4HZpbnLx3rdo9wfz8OnFKmu4BHgJ3Guu1t9vsNwIHA6gHmd/z9ayKfITTzdRiLgfNduRF4saQ9R7uhHTRkn21/1/aj5emNVJ/5GM+a/dqT/wVcBjw0mo0bIc30+d3A5bbvB7A9nvvdTH8NvEiSgBdSBcLm0W1mZ9m+nqofA+n4+9dEDoRGX4cxrYVlxpPh9ucYqiOM8WzIPkuaBrwDOGMU2zWSmvk9vwLYTdK1km6RtHTUWtd5zfT3c8CrqD7QehvwYdvPjk7zxkzH37+2ic8hjJBmvg6jqa/MGEea7o+kN1EFwu+OaItGXjN9/izwMdtbqgPIca+ZPk8GDgIWAjsDN0i60faPR7pxI6CZ/h4KrAJ+D3g5cLWkb9t+YoTbNpY6/v41kQOhma/DmGhfmdFUfyS9Gvg8cJjtn41S20ZKM32eB1xUwmAqcLikzba/Miot7Lxm/7Yftv0U8JSk64HXAOMxEJrp7/uBk10NrvdIugd4JXDz6DRxTHT8/WsiDxk183UYK4Gl5Wr9AuBx2+tHu6EdNGSfJc0ELgfeO06PFvsbss+297E9y/Ys4FLgf47jMIDm/ravAA6WNFnS86m+PfiOUW5npzTT3/upzoaQtAewH3D3qLZy9HX8/WvCniF4gK/DkHRsmX8G1R0nhwM9wNNURxnjVpN9/ltgd+C0csS82eP4myKb7POE0kyfbd8h6RvArcCzwOdtN7x9cVvX5O/474FzJd1GNZTyMdvj+iuxJV0IHAJMldQLfALYEUbu/StfXREREcDEHjKKiIhhSCBERASQQIiIiCKBEBERQAIhIiKKBEJERAAJhIiIKP4b70kaD7uBLycAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Histogram of feature title_sentiment_polarity after the scaling\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We make a copy of the original pandas DataFrame\n",
    "online_news_popularity_scaled = online_news_popularity_upsampled.copy()\n",
    "\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0,n_quantiles=112, output_distribution='normal')\n",
    "\n",
    "#The following lines are used to scale only numerical features, in fact we don't have categorical features\n",
    "for el in online_news_popularity_scaled.columns:\n",
    "    if(online_news_popularity_scaled[el].dtype == 'int64' or online_news_popularity_scaled[el].dtype == 'float64') and el != ' shares':\n",
    "        #We can't pass a 1D array to quantile_transformer.fit_transform() and so we have to reshape\n",
    "        #with the following easy way\n",
    "        reshape = online_news_popularity_scaled[el].values.reshape(-1,1)\n",
    "        online_news_popularity_scaled[el] = quantile_transformer.fit_transform(reshape)\n",
    "\n",
    "#Now we make an example polot of a feature to represent the scaling\n",
    "#We show how was the distribution before the scaling process\n",
    "print(\"Histogram of feature title_sentiment_polarity before the scaling\")\n",
    "plt.hist(online_news_popularity[' title_subjectivity'])\n",
    "plt.title('title_subjectivity - Before scaling')\n",
    "plt.show()\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "#We show how is the distribution after the scaling process\n",
    "print(\"Histogram of feature title_sentiment_polarity after the scaling\")\n",
    "plt.hist(online_news_popularity_scaled[' title_subjectivity'])\n",
    "plt.title('title_subjectivity - After scaling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we show the code to normalize numerical features by <i>l2 norm<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three values of feature title_sentiment_polarity after the scaling but before the normalization\n",
      "0   -0.853480\n",
      "1   -0.650584\n",
      "2   -0.901039\n",
      "Name:  n_tokens_content, dtype: float64\n",
      "------------------------------------------------------------------\n",
      "First three values of feature title_sentiment_polarity after the scaling and after the normalization\n",
      "0   -0.003238\n",
      "1   -0.002468\n",
      "2   -0.003418\n",
      "Name:  n_tokens_content, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We make a copy of the scaled pandas dataframe\n",
    "online_news_popularity_norm = online_news_popularity_scaled.copy()\n",
    "\n",
    "#The following lines are used to normalize only numerical features\n",
    "for el in online_news_popularity_norm.columns:\n",
    "    if(online_news_popularity_norm[el].dtype == 'int64' or online_news_popularity_norm[el].dtype == \"float64\") and el != ' shares':\n",
    "        #We can't pass a 1D array to the function normalize() and so we have to reshape with the following easy way\n",
    "        reshape = online_news_popularity_norm[el].values.reshape(-1,1)\n",
    "        reshape = preprocessing.normalize([online_news_popularity_norm[el]], norm='l2')\n",
    "        online_news_popularity_norm[el] = reshape[0]\n",
    "\n",
    "#We show the first three values of a feature before the normalization\n",
    "print(\"First three values of feature title_sentiment_polarity after the scaling but before the normalization\")\n",
    "print(online_news_popularity_scaled[' n_tokens_content'].head(3))\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#We show the first three values of feature after the normalization\n",
    "print(\"First three values of feature title_sentiment_polarity after the scaling and after the normalization\")\n",
    "print(online_news_popularity_norm[' n_tokens_content'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now contains only numeric features therefore we do not need to code our features using the methods such as SimpleEncode, OneHotEncode, get_dummies and so on. We are ready to define the class to predict y and all the other features X. Now we show the code to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/xt97zly148d7d9kycf5fxb_00000gn/T/ipykernel_56300/2005847261.py:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_MS = online_news_popularity_MS.drop([' shares'],1)\n",
      "/var/folders/ff/xt97zly148d7d9kycf5fxb_00000gn/T/ipykernel_56300/2005847261.py:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_Worst = online_news_popularity_Worst.drop([' shares'],1)\n"
     ]
    }
   ],
   "source": [
    "#We make 2 copy, one of the normalized, scaled and class balanced data and one of the original one in other\n",
    "#to compare the result after\n",
    "#for reasons related to execution times I have performed the comparison between the processed data and not only with the decision tree\n",
    "online_news_popularity_MS = online_news_popularity_norm.copy()\n",
    "online_news_popularity_Worst = online_news_popularity.copy()\n",
    "\n",
    "#We define the class to predict, y, and all the other features, X\n",
    "y_MS = online_news_popularity_MS[' shares']\n",
    "X_MS = online_news_popularity_MS.drop([' shares'],1)\n",
    "\n",
    "y_Worst = online_news_popularity_Worst[' shares']\n",
    "X_Worst = online_news_popularity_Worst.drop([' shares'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to answer to the question about <b>Feature Importance</b>. Only for Decision Tree, we fit another model to answer to this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.041138\n",
      "Feature 1: 0.021293\n",
      "Feature 2: 0.019559\n",
      "Feature 3: 0.024220\n",
      "Feature 4: 0.017964\n",
      "Feature 5: 0.024953\n",
      "Feature 6: 0.019754\n",
      "Feature 7: 0.014795\n",
      "Feature 8: 0.013433\n",
      "Feature 9: 0.008180\n",
      "Feature 10: 0.031047\n",
      "Feature 11: 0.007054\n",
      "Feature 12: 0.001163\n",
      "Feature 13: 0.006586\n",
      "Feature 14: 0.003003\n",
      "Feature 15: 0.004745\n",
      "Feature 16: 0.007520\n",
      "Feature 17: 0.002553\n",
      "Feature 18: 0.001794\n",
      "Feature 19: 0.024702\n",
      "Feature 20: 0.026313\n",
      "Feature 21: 0.015019\n",
      "Feature 22: 0.001520\n",
      "Feature 23: 0.032790\n",
      "Feature 24: 0.020337\n",
      "Feature 25: 0.031568\n",
      "Feature 26: 0.048185\n",
      "Feature 27: 0.025194\n",
      "Feature 28: 0.018861\n",
      "Feature 29: 0.022463\n",
      "Feature 30: 0.004912\n",
      "Feature 31: 0.003529\n",
      "Feature 32: 0.003852\n",
      "Feature 33: 0.004041\n",
      "Feature 34: 0.004474\n",
      "Feature 35: 0.000949\n",
      "Feature 36: 0.000659\n",
      "Feature 37: 0.008895\n",
      "Feature 38: 0.028163\n",
      "Feature 39: 0.025668\n",
      "Feature 40: 0.027382\n",
      "Feature 41: 0.025177\n",
      "Feature 42: 0.028815\n",
      "Feature 43: 0.030383\n",
      "Feature 44: 0.024350\n",
      "Feature 45: 0.026667\n",
      "Feature 46: 0.025135\n",
      "Feature 47: 0.013888\n",
      "Feature 48: 0.013608\n",
      "Feature 49: 0.029218\n",
      "Feature 50: 0.013179\n",
      "Feature 51: 0.011575\n",
      "Feature 52: 0.025908\n",
      "Feature 53: 0.015065\n",
      "Feature 54: 0.016577\n",
      "Feature 55: 0.013253\n",
      "Feature 56: 0.012463\n",
      "Feature 57: 0.012793\n",
      "Feature 58: 0.011716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 59 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We learn a model only on the processed data\n",
    "clf_FI = tree.DecisionTreeClassifier()\n",
    "clf_FI.fit(X_MS, y_MS)\n",
    "\n",
    "# We store our importances in a list\n",
    "importance = clf_FI.feature_importances_\n",
    "\n",
    "# We define the list of number \n",
    "number_list = [number for number in range(len(importance))]\n",
    "\n",
    "# We print the importance of each feature\n",
    "for i in range (len(importance)):\n",
    "    print('Feature %d: %f' % (i, importance[i]))\n",
    "\n",
    "# We plot the feature importance obtained\n",
    "pyplot.bar(number_list, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze the importance of all the features and we drop all the features with importance <= 0.01 because they are useless for the train.\n",
    "Now we show te code to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>-0.003238</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>-0.003259</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>-0.006348</td>\n",
       "      <td>0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024312</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>-0.002468</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>-0.002568</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>-0.003433</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.006678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024312</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>-0.003418</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>-0.003694</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>-0.003792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.006678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024312</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.006678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>-0.005194</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.007366</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44110</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.006945</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>-0.001821</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>-0.000866</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44111</th>\n",
       "      <td>-0.008803</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>-0.004190</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000650</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112</th>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>-0.001921</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>-0.001237</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000910</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.006678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44113</th>\n",
       "      <td>-0.002401</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>-0.001488</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44114</th>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.004340</td>\n",
       "      <td>-0.002633</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44115 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "0        0.024312         0.003695          -0.003238          0.004524   \n",
       "1        0.024312        -0.003199          -0.002468          0.002442   \n",
       "2        0.024312        -0.003199          -0.003418          0.001325   \n",
       "3        0.024312        -0.003199           0.001175         -0.001358   \n",
       "4        0.024312         0.005938           0.004826         -0.004624   \n",
       "...           ...              ...                ...               ...   \n",
       "44110    0.000051        -0.006945           0.001378         -0.001821   \n",
       "44111   -0.008803         0.004870          -0.000112          0.004008   \n",
       "44112    0.000769         0.000327          -0.001921          0.001406   \n",
       "44113   -0.002401         0.002482           0.003473          0.000802   \n",
       "44114    0.000595        -0.004340          -0.002633          0.003906   \n",
       "\n",
       "        n_non_stop_words   n_non_stop_unique_tokens   num_hrefs  \\\n",
       "0              -0.003259                   0.004819   -0.002925   \n",
       "1              -0.002568                   0.003996   -0.004108   \n",
       "2              -0.003694                  -0.001116   -0.004108   \n",
       "3               0.001296                  -0.001042    0.000806   \n",
       "4               0.005459                  -0.005194    0.003957   \n",
       "...                  ...                        ...         ...   \n",
       "44110           0.001037                  -0.000539    0.000041   \n",
       "44111          -0.004190                   0.003800   -0.000732   \n",
       "44112          -0.002198                   0.000881    0.001842   \n",
       "44113          -0.000700                   0.003275   -0.001488   \n",
       "44114          -0.002672                   0.002313    0.002563   \n",
       "\n",
       "        num_self_hrefs   num_imgs   average_token_length  ...  \\\n",
       "0            -0.000594  -0.000537               0.000231  ...   \n",
       "1            -0.001893  -0.000537               0.003375  ...   \n",
       "2            -0.001893  -0.000537              -0.003792  ...   \n",
       "3            -0.012298  -0.000537              -0.003644  ...   \n",
       "4             0.005458   0.003531               0.000266  ...   \n",
       "...                ...        ...                    ...  ...   \n",
       "44110         0.003112   0.001475              -0.004108  ...   \n",
       "44111        -0.001231  -0.002067              -0.000687  ...   \n",
       "44112        -0.001237   0.001998               0.003732  ...   \n",
       "44113        -0.000030   0.000675              -0.002185  ...   \n",
       "44114         0.000457   0.002174              -0.000080  ...   \n",
       "\n",
       "        global_subjectivity   global_rate_positive_words  \\\n",
       "0                  0.003230                     0.001581   \n",
       "1                 -0.004593                     0.000994   \n",
       "2                  0.009030                     0.003983   \n",
       "3                 -0.001099                     0.000589   \n",
       "4                  0.002860                     0.007366   \n",
       "...                     ...                          ...   \n",
       "44110              0.000459                     0.002314   \n",
       "44111              0.000601                     0.002730   \n",
       "44112              0.001466                     0.002246   \n",
       "44113             -0.001325                     0.002685   \n",
       "44114              0.007678                     0.008769   \n",
       "\n",
       "        rate_positive_words   avg_positive_polarity   max_positive_polarity  \\\n",
       "0                  0.001356                0.000989               -0.000437   \n",
       "1                  0.000553               -0.003433               -0.000437   \n",
       "2                  0.003536                0.005972                0.007628   \n",
       "3                 -0.000864                0.001348                0.000050   \n",
       "4                  0.003661                0.002576                0.007628   \n",
       "...                     ...                     ...                     ...   \n",
       "44110              0.000238                0.000547                0.000330   \n",
       "44111             -0.000383                0.000033               -0.000650   \n",
       "44112              0.000794                0.004481               -0.000286   \n",
       "44113             -0.001227               -0.000957               -0.000562   \n",
       "44114              0.000232                0.004196                0.000554   \n",
       "\n",
       "        min_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.000632             0.000904                  -0.005935   \n",
       "1                    0.002794            -0.006757                  -0.001120   \n",
       "2                   -0.001833            -0.006757                  -0.001120   \n",
       "3                   -0.000632            -0.006757                  -0.001120   \n",
       "4                    0.000092             0.000692                   0.002610   \n",
       "...                       ...                  ...                        ...   \n",
       "44110               -0.000866            -0.000038                   0.001996   \n",
       "44111               -0.002277            -0.000022                   0.007743   \n",
       "44112               -0.000910            -0.006757                  -0.001120   \n",
       "44113               -0.000912             0.001022                   0.007233   \n",
       "44114               -0.001014             0.006757                   0.008283   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity  \n",
       "0                    -0.006348                       0.000618  \n",
       "1                     0.006348                      -0.006678  \n",
       "2                     0.006348                      -0.006678  \n",
       "3                     0.006348                      -0.006678  \n",
       "4                    -0.001571                       0.000413  \n",
       "...                        ...                            ...  \n",
       "44110                -0.000105                       0.000167  \n",
       "44111                -0.000270                       0.002195  \n",
       "44112                 0.006348                      -0.006678  \n",
       "44113                 0.006348                       0.001967  \n",
       "44114                 0.006348                       0.002435  \n",
       "\n",
       "[44115 rows x 41 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We drop the columns (features) with importance <= 0.01\n",
    "for i in range (len(importance)):\n",
    "    if (importance[i] <= 0.01):\n",
    "        X_MS.drop(X_MS.columns[[i]], axis=1, inplace=True)\n",
    "X_MS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessing phase is now terminated, our dataset not have missing values, the target class is now discretized and the classes are balanced. We are ready to start the next step: <b>Model Selection<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to experiment different models to predict the target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to split our X and Y into train and test sets. We will do this for both processed and unprocessed data (we are going to use unprocessed data only for decision tree classifier in order to make a comparison). Now we show the code to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the function that splits the pandas dataframe, y_train and y_test contain the ground \n",
    "# truth respectively for X_train and X_test\n",
    "X_train_MS, X_test_MS, y_train_MS, y_test_MS = train_test_split(X_MS, y_MS, test_size=0.2, random_state = 112)\n",
    "\n",
    "X_train_Worst, X_test_Worst, y_train_Worst, y_test_Worst = train_test_split(X_Worst, y_Worst,\n",
    "                                                                            test_size=0.2, random_state = 112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discretize our target with 5 bins we can see that the values of our y_train_MS are between 0.0 and 4.0. We can check it by print y_train_MS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30658    0.0\n",
       "3087     4.0\n",
       "42905    3.0\n",
       "7519     3.0\n",
       "26351    1.0\n",
       "        ... \n",
       "38085    0.0\n",
       "18753    2.0\n",
       "7642     4.0\n",
       "232      1.0\n",
       "43947    4.0\n",
       "Name:  shares, Length: 35292, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We print the values of our target\n",
    "y_train_MS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now split the two parts, the training set, and the test set, where the training set contains 80% of samples of the dataset normalized, scaled, and with class balanced, and the test set contains the remaining 20%. We are going to learn a model on the training set and then, do the prediction on the test set. We will see the results of the prediction phase in the model evaluation section where we evaluate these results. Moreover, in that section, we will show the difference with the prediction done on data not scaled, normalized, and without balanced class.\n",
    "\n",
    "We can use algorithms like Decision Tree Classifier, SVM classifier, and Random Forest that can't handle categorical features but in our case we don't have categorical feature, this is just a consideration. Moreover, if the classes to predict are not balanced this can impact the performance evaluation of some learned model (e.g. Decision Tree, SVM, and Random Forest), but we have already balanced our classes and then this problem won't occur. Finally, we are going to use scaled and normalized data, if we do not scale and normalize the data this can impact the performance evaluation of a Multi Layer Perceptron Networks Classifier. Note that even if we use scaled and normalized data in other models where this is not required, the performance evaluation won't be worst. I decided so to do the split on data with balanced classes and with data scaled and normalized.\n",
    "\n",
    "For each selected model we also do hyperparameter tuning to define the best parameter for each model. The grid search was provided by GridSearchCV that exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter and also was used the <i>RandomizedSearchCV</i> function that implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "Decision Tree is a supervised machine learning algorithm that creates a tree with three important components:\n",
    "- Test node: that is a test on a specific feature;\n",
    "- Leaf node: that represents a decision (classification);\n",
    "- Edge: that can link two test nodes or one test node to a leaf node.\n",
    "A decision tree can be re-written as a set of rules and the \"function\" to minimize is the depth of the tree, but is a NP-Hard problem and so a Greedy technique is even good. One possibility is to define \"the order\" of testing the different features, based for example on the information gain that is based on the entropy that represents the \"disorder\" of the set (of instances) that we are using.\n",
    "\n",
    "We are going to show the necessary code to run a Decision Tree Classifier. We also know the code to find the best parameter for this model.\n",
    "\n",
    "As we said int the previous lines, classed are already balanced and we don't need to encode our feature because they'are all numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for scaled, normalized and class balanced data: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 8}\n",
      "-----------------------------------------------\n",
      "Best parameters for original data: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 9}\n"
     ]
    }
   ],
   "source": [
    "#We define the parameter to tune, in particular we are making tuning on the criterion, on the minimum number\n",
    "#of samples for a split and the minimum number of sampled in a leaf\n",
    "parameter_DTC = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': range(1, 10),\n",
    "    'max_depth': [int(x) for x in range(10, 100, 10)]}\n",
    "\n",
    "\n",
    "#Now we use the function GridSearchCV in order to find the best parameter from the ones that we want to tune\n",
    "clf_MS = GridSearchCV(tree.DecisionTreeClassifier(), parameter_DTC, n_jobs = -1) \n",
    "#n_jobs = -1 allow the parallelization\n",
    "\n",
    "#Now we stat to learn the model\n",
    "clf_MS.fit(X_train_MS, y_train_MS) \n",
    "    \n",
    "#We now show the best parameter for processed data\n",
    "best_params_DTC_MS = clf_MS.best_params_\n",
    "\n",
    "print(\"Best parameters for scaled, normalized and class balanced data:\", best_params_DTC_MS)\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "#We now show the best parameter for not processed data\n",
    "clf_Worst = GridSearchCV(tree.DecisionTreeClassifier(), parameter_DTC, n_jobs = -1) \n",
    "clf_Worst.fit(X_train_Worst, y_train_Worst)\n",
    "best_params_DTC_Worst = clf_Worst.best_params_\n",
    "\n",
    "print(\"Best parameters for original data:\", best_params_DTC_Worst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done hyperparameter tuning on the <i>criterion</i>, on <i>the minimum number of samples in a leaf</i> and then we show what are the best parameter values that maximize the performance of the model. So in the prediction phase, we are going to use these parameters, to obtain the maximum score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a supervised machine learning algorithm and it aims to find a hyperplane that well separates instances, to categorize future instances. In particular, it finds the best separating line that maximizes at best the margin. Have a small margin means that we can fall into the problem of overfitting, instead have a large margin means that we have a more general model but also that can we do more misclassification.\n",
    "\n",
    "As for the Decision Tree Classifier, we show the necessary code to train a model and to obtain the best parameter for this model, and also classes are already balanced. Most of the code is similar to the previous ones and so we shortly comment on the code.\n",
    "\n",
    "One important thing done for the SVM classifier is to remove categorical features, since SVM is a distance based model and categorical feature do not \"help\" to much to defines the \"closeness\" of two instances, in particular, the fact is that in some how is impossible to define some notion of distance between two categorical values.\n",
    "\n",
    "Classes are also already balanced and since we remove categorical features we do not need to encode nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for scaled, normalized and class balanced data: {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# We use a copy of the original splitted data\n",
    "X_num_MS = X_MS.copy()\n",
    "X_num_Worst = X_Worst.copy()\n",
    "        \n",
    "X_train_SVM_MS, X_test_SVM_MS, y_train_SVM_MS, y_test_SVM_MS = train_test_split(X_num_MS, y_MS,\n",
    "                                                                                test_size=0.2, random_state = 112)\n",
    "\n",
    "X_train_SVM_Worst, X_test_SVM_Worst, y_train_SVM_Worst, y_test_SVM_Worst = train_test_split(X_num_Worst, y_Worst,\n",
    "                                                                            test_size=0.2, random_state = 112)\n",
    "\n",
    "\n",
    "#We want to do hyper parameter tuning on kernel, gamma value and C value\n",
    "parameters_SVM = {\n",
    "    'kernel': ['linear'], # we use only linear because rbf is very slow!\n",
    "    'gamma': [1e-3, 1e-4],\n",
    "    'C': [1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "#We start our Grid Search\n",
    "svc_MS = GridSearchCV(svm.SVC(), parameters_SVM, n_jobs=-1)\n",
    "svc_MS.fit(X_train_SVM_MS, y_train_SVM_MS)  \n",
    "\n",
    "#We now show the best parameter for processed data\n",
    "best_params_SVC_MS = svc_MS.best_params_\n",
    "print(\"Best parameters for scaled, normalized and class balanced data:\", best_params_SVC_MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the best parameter (from the ones that we selected) for SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble method: Random Forest\n",
    "The ensamble method is a supervised machine learning algorithm that is represented by a set of classifiers that learn a target function and combine the prediction of the single classifier to predict unseen instances. This algorithm reduces the variance and improves the generalization and the Random Forest algorithm is one of the techniques to manipulate the Data set. In particular, it uses a set of decision trees, learned with the random vector method, and then combines the prediction with one of the two voting methods.\n",
    "\n",
    "In the following, we are going to show the necessary code to train a model and to obtain the best parameter for this model. Random forest made use of Decision Trees and so as for Decision Tree Classifier we need balanced classes and to encode categorical features into numerical ones, but this was already done before, and so we are ready to learn the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for scaled, normalized and class balanced data: {'max_depth': 60, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "#We want to do hyper parameter tuning on the minimum number of samples for a split, on the minimum number \n",
    "#of sampled in a leaf and on the number of estimators\n",
    "parameter_RF = {\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],\n",
    "    'max_depth': [int(x) for x in range(10, 100, 10)]}\n",
    "\n",
    "#We start our Grid Search\n",
    "rf_MS = GridSearchCV(RandomForestClassifier(), parameter_RF, n_jobs=-1)\n",
    "rf_MS.fit(X_train_MS, y_train_MS) \n",
    "\n",
    "#We now show the best parameter for processed data\n",
    "best_params_RF_MS = rf_MS.best_params_\n",
    "print(\"Best parameters for scaled, normalized and class balanced data:\", best_params_RF_MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the best parameter (from the ones that we selected) for Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Network Classifier\n",
    "MLPNs is a supervised machine learning algorithm and it made use of perceptrons. The structure of MLPNs is composed of input, hidden and output layers fully connected one to the other with a feed-forward activation function. It made use of a Convolutional function to \"active\" or not one node (perceptron). In MLPNs we want that our outputs are closer as possible to the right one, and so we have to iteratively adjust the weights of each edge.\n",
    "\n",
    "In the following, we are going to show the necessary code to train a model and to obtain the best parameter for this model. Multi Layer Perceptron Networks Classifier need data scaled and normalized but as we said at the beginning of the section we decided to use scaled and normalized data and so we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for scaled, normalized and class balanced data: {'tol': 0.0001, 'solver': 'adam', 'hidden_layer_sizes': [12, 8], 'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#We want to do hyper parameter tuning on size of the hidden layers, on the alpha value, on the solver and on the tol (this for optimization)\n",
    "parameter_MLP = {\n",
    "    'hidden_layer_sizes' : [[12,8], [15,10], [20,15]],\n",
    "    'alpha' : [1e-2, 1e-4, 1e-6],\n",
    "    'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "    'tol': [1e-2, 1e-4]}\n",
    "\n",
    "#We start our Randomized Search\n",
    "mlpn_MS = RandomizedSearchCV(MLPClassifier(), parameter_MLP, n_jobs=-1)\n",
    "mlpn_MS.fit(X_train_MS, y_train_MS)\n",
    "\n",
    "#We now show the best parameter for processed data\n",
    "best_params_MLP_MS = mlpn_MS.best_params_\n",
    "print(\"Best parameters for scaled, normalized and class balanced data:\", best_params_MLP_MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for some cases the algorithm does not converge, in fact it reaches the maximum of iterations (by defaut it is <b>200</b>, as specified in the warnings above). For this reason i decided to use the <b>tol</b> parameter in order to increase the tolerance for optimization.\\\n",
    "We now have the best parameter (from the ones that we selected) for MLPNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section for each model selected we first show the confusion matrix and the classification report obtained from the previous model learned and then with the function <i>cross_val_score</i> we split the data, fit a model and compute the score (we evaluate the model on the accuracy) 5 consecutive times and then we output the mean of all the scores obtained +/- the standard deviation. In this way, we reduce the probability to be \"lucky\" or \"unlucky\" by splitting the dataset into training and test set, and also by adding \"+/- the standard deviation\" we define the Confidence interval of our accuracy. Moreover, we will also compare the results obtained from not processed data and processed ones (only for Decision Tree as a matter of execution time). We output also the max of all the scores obtained because the authors of the paper use the max instead of the mean in their results.\n",
    "\n",
    "As for the previous section we will use data with balanced classes and data scaled and normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to show the confusion matrix and the classification report on the prediction done on the test set <i>test_enc</i> after the model trained in the previous section. Notice that the best parameter obtained is already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED DATA:\n",
      "Confusion Matrix: \n",
      "[[961 237 228 119 223]\n",
      " [676 345 309 206 228]\n",
      " [533 195 383 303 345]\n",
      " [410 154 371 391 435]\n",
      " [373 142 276 328 652]]\n",
      "------------------------------------------\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.33      0.54      0.41      1768\n",
      "     Class 1       0.32      0.20      0.24      1764\n",
      "     Class 2       0.24      0.22      0.23      1759\n",
      "     Class 3       0.29      0.22      0.25      1761\n",
      "     Class 4       0.35      0.37      0.36      1771\n",
      "\n",
      "    accuracy                           0.31      8823\n",
      "   macro avg       0.31      0.31      0.30      8823\n",
      "weighted avg       0.31      0.31      0.30      8823\n",
      "\n",
      "-----------------------------------------------\n",
      "NOT PROCESSED DATA:\n",
      "Confusion Matrix: \n",
      "[[817 109 314 107 234]\n",
      " [551 113 303 143 249]\n",
      " [484 134 441 292 429]\n",
      " [344 107 398 295 453]\n",
      " [283  72 290 276 690]]\n",
      "------------------------------------------\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.33      0.52      0.40      1581\n",
      "     Class 1       0.21      0.08      0.12      1359\n",
      "     Class 2       0.25      0.25      0.25      1780\n",
      "     Class 3       0.27      0.18      0.22      1597\n",
      "     Class 4       0.34      0.43      0.38      1611\n",
      "\n",
      "    accuracy                           0.30      7928\n",
      "   macro avg       0.28      0.29      0.27      7928\n",
      "weighted avg       0.28      0.30      0.28      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We define a list of target names to assigne to each classes\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "#We predict the class for each samples in the test set\n",
    "predictedDTC_MS = clf_MS.predict(X_test_MS)\n",
    "#We print the confusion matrix and the classification Report for processed data\n",
    "print(\"PROCESSED DATA:\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test_MS, predictedDTC_MS))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test_MS, predictedDTC_MS, target_names = target_names))\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"NOT PROCESSED DATA:\")\n",
    "\n",
    "predictedDTC_Worst = clf_Worst.predict(X_test_Worst)\n",
    "#We print the confusion matrix and the classification Report for not processed data\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test_Worst, predictedDTC_Worst))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test_Worst, predictedDTC_Worst, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can see that for Class 4 in process data we obtain a higher precision (35%) but in Class 0 we have a higher f1-score (41%) that represents a trade-off between precision and recall.\\\n",
    "Also we can observe that the accuracy is 31%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the necessary code to perform cross-validation on the dataset. In particular, we are going to evaluate the accuracy that we obtain from each test set. I decided to do the cross-validation on the processed data because as we can see they have better performances than the not processed ones. Classes are so already balanced and then we don't need to encode categorical features as we descard them We are going to use also the best parameter tuned in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/xt97zly148d7d9kycf5fxb_00000gn/T/ipykernel_56300/157775035.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_ME = online_news_popularity_ME.drop(' shares',1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy: 0.21  Mean Accuracy: 0.12 (+/- 0.12)\n"
     ]
    }
   ],
   "source": [
    "# We will use a copy of the original dataset\n",
    "online_news_popularity_ME = online_news_popularity_norm.copy()\n",
    "\n",
    "# We define the class to predict, y, and all the other features, X.\n",
    "y_ME = online_news_popularity_ME[' shares']\n",
    "X_ME = online_news_popularity_ME.drop(' shares',1)\n",
    "\n",
    "\n",
    "# We set the best parameter to the Decision Tree Classifier\n",
    "clfCV = tree.DecisionTreeClassifier(criterion = best_params_DTC_MS['criterion'],\n",
    "                                    min_samples_leaf = best_params_DTC_MS['min_samples_leaf'],\n",
    "                                    max_depth = best_params_DTC_MS['max_depth'])\n",
    "\n",
    "# We do the cross validation and we store the scores into a list, if not specified the score that we will calculate\n",
    "# is the accuracy\n",
    "scores_DTC = cross_val_score(clfCV, X_ME, y_ME, cv=5, n_jobs=-1)\n",
    "\n",
    "# We print the results that we obtain\n",
    "print(\"Max Accuracy: %0.2f\" % (scores_DTC.max()), \" Mean Accuracy: %0.2f (+/- %0.2f)\" % (scores_DTC.mean(), scores_DTC.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy that we obtain is lower than the accuracy that we printed in the classification report, 0.31, but as we said the cross-validation operation is used to reduce the probability to be unlucky and lucky by splitting the dataset. We may have been lucky by splitting the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to show the confusion matrix and the classification report on the prediction done on the test set <i>X_test_num</i> after the model trained in the previous section. Most of the operations are similar and so the comment on the code will be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED DATA:\n",
      "Confusion Matrix: \n",
      "[[849 289  85 268 277]\n",
      " [523 416 136 393 296]\n",
      " [423 243 142 543 408]\n",
      " [313 192 137 681 438]\n",
      " [312 148  62 530 719]]\n",
      "------------------------------------------\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.35      0.48      0.41      1768\n",
      "     Class 1       0.32      0.24      0.27      1764\n",
      "     Class 2       0.25      0.08      0.12      1759\n",
      "     Class 3       0.28      0.39      0.33      1761\n",
      "     Class 4       0.34      0.41      0.37      1771\n",
      "\n",
      "    accuracy                           0.32      8823\n",
      "   macro avg       0.31      0.32      0.30      8823\n",
      "weighted avg       0.31      0.32      0.30      8823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We do the prediction on X_test_num (DataSet that contain only numerical features)\n",
    "predictedSVM_MS = svc_MS.predict(X_test_SVM_MS) \n",
    "#We print the confusion matrix and the classification Report for processed data\n",
    "print(\"PROCESSED DATA:\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test_SVM_MS, predictedSVM_MS))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test_SVM_MS, predictedSVM_MS, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can see that for Class 0 in process data we obtain a higher precision and a f1-score that represents a trade-off between precision and recall.\\\n",
    "Also we can observe that the accuracy is 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy: 0.30  Mean Accuracy: 0.27 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "# We set the best parameter for SVM\n",
    "svcCV = svm.SVC(kernel = best_params_SVC_MS['kernel'], gamma = best_params_SVC_MS['gamma'],\n",
    "                C = best_params_SVC_MS['C'])\n",
    "\n",
    "#We apply the croos validation and store the results in a list\n",
    "scores_SVC = cross_val_score(svcCV, X_ME, y_ME, cv=5, n_jobs=-1)\n",
    "\n",
    "#We print the results that we obtain\n",
    "print(\"Max Accuracy: %0.2f\" % (scores_SVC.max()), \" Mean Accuracy: %0.2f (+/- %0.2f)\" % (scores_SVC.mean(), scores_SVC.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy that we obtain is lower than the accuracy that we printed in the classification report, 0.32, but as we said the cross-validation operation is used to reduce the probability to be unlucky and lucky by splitting the dataset. We may have been lucky by splitting the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to show the confusion matrix and the classification report on the prediction done on the test set <i>test_enc</i> after the model trained in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED DATA:\n",
      "Confusion Matrix: \n",
      "[[938 292 195 108 235]\n",
      " [420 693 254 158 239]\n",
      " [384 220 389 354 412]\n",
      " [224 186 302 598 451]\n",
      " [214 134 204 325 894]]\n",
      "------------------------------------------\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.43      0.53      0.48      1768\n",
      "     Class 1       0.45      0.39      0.42      1764\n",
      "     Class 2       0.29      0.22      0.25      1759\n",
      "     Class 3       0.39      0.34      0.36      1761\n",
      "     Class 4       0.40      0.50      0.45      1771\n",
      "\n",
      "    accuracy                           0.40      8823\n",
      "   macro avg       0.39      0.40      0.39      8823\n",
      "weighted avg       0.39      0.40      0.39      8823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We predict the class for each samples in the test set\n",
    "predictedRF_MS = rf_MS.predict(X_test_MS)\n",
    "#We print the confusion matrix and the classification Report for processed data\n",
    "print(\"PROCESSED DATA:\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test_MS, predictedRF_MS))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test_MS, predictedRF_MS, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can see that for Class 0 in process data we obtain a higher precision and a f1-score that represents a trade-off between precision and recall.\\\n",
    "Also we can observe that the accuracy is 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy: 0.34  Mean Accuracy: 0.24 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "#We set the best parameter for Random Forest\n",
    "rfCV = RandomForestClassifier(n_estimators = best_params_RF_MS['n_estimators'], \n",
    "                                min_samples_leaf = best_params_RF_MS['min_samples_leaf'],\n",
    "                                min_samples_split = best_params_RF_MS['min_samples_split'],\n",
    "                                max_depth = best_params_RF_MS['max_depth'])\n",
    "\n",
    "#We apply the croos validation and store the results in a list\n",
    "scores_RF = cross_val_score(rfCV, X_ME, y_ME, cv=5, n_jobs=-1)\n",
    "\n",
    "#We print the results that we obtain\n",
    "print(\"Max Accuracy: %0.2f\" % (scores_RF.max()), \" Mean Accuracy: %0.2f (+/- %0.2f)\" % (scores_RF.mean(), scores_RF.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy that we obtain is lower than the accuracy that we printed in the classification report, 0.40, but as we said the cross-validation operation is used to reduce the probability to be unlucky and lucky by splitting the dataset. We may have been lucky by splitting the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to show the confusion matrix and the classification report on the prediction done on the test set test_enc after the model trained in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED DATA:\n",
      "Confusion Matrix: \n",
      "[[1122  154  100   77  315]\n",
      " [ 903  188  160  161  352]\n",
      " [ 581  173  234  260  511]\n",
      " [ 415  137  172  409  628]\n",
      " [ 394   88  122  282  885]]\n",
      "------------------------------------------\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.33      0.63      0.43      1768\n",
      "     Class 1       0.25      0.11      0.15      1764\n",
      "     Class 2       0.30      0.13      0.18      1759\n",
      "     Class 3       0.34      0.23      0.28      1761\n",
      "     Class 4       0.33      0.50      0.40      1771\n",
      "\n",
      "    accuracy                           0.32      8823\n",
      "   macro avg       0.31      0.32      0.29      8823\n",
      "weighted avg       0.31      0.32      0.29      8823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We predict the class for each samples in the test set\n",
    "predictedMLP_MS = mlpn_MS.predict(X_test_MS)\n",
    "#We print the confusion matrix and the classification Report for processed data\n",
    "print(\"PROCESSED DATA:\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test_MS, predictedMLP_MS))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test_MS, predictedMLP_MS, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can see that for Class 4 and Class 0 in process data we obtain a higher precision (33%), but the latter has a higher f1-score that represents a trade-off between precision and recall.\\\n",
    "Also we can observe that the precision is 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy: 0.32  Mean Accuracy: 0.27 (+/- 0.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#We set the best parameter for MLPNs\n",
    "mlpnCV = MLPClassifier(solver = best_params_MLP_MS['solver'],\n",
    "                    hidden_layer_sizes = best_params_MLP_MS['hidden_layer_sizes'],\n",
    "                    alpha = best_params_MLP_MS['alpha'],\n",
    "                    tol = best_params_MLP_MS['tol'])\n",
    "\n",
    "#We apply the croos validation and store the results in a list\n",
    "scores_MLP = cross_val_score(mlpnCV, X_ME, y_ME, cv=5, n_jobs=-1)\n",
    "\n",
    "#We print the results that we obtain\n",
    "print(\"Max Accuracy: %0.2f\" % (scores_MLP.max()), \" Mean Accuracy: %0.2f (+/- %0.2f)\" % (scores_MLP.mean(), scores_MLP.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy that we obtain is lower than the accuracy that we printed in the classification report, 0.32, but as we said the cross-validation operation is used to reduce the probability to be unlucky and lucky by splitting the dataset. We may have been lucky by splitting the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here your comments for step 3 Model Selection\n",
    "\n",
    "### Comments step 3\n",
    "In this step I initially decided to conclude the pre processing phase started in the first part, then going to discretize the target, apply the scaling and do the normalization and in the end respond to the question about Feature Importance. Then I did the train of the four models specified in the delivery (Decision Tree, SVM, Random Forest and MLPNs classifier) and did the evaluation phase. After that I observed the results using the confusion matrx and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4. Summary\n",
    "Provide a summary discussion (in English) of your solution <b>(at least 500 words)</b> feel free to include plots figures and tables.\n",
    "\n",
    "<b>This is a mandatory step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here <b>your own</b> summary dicussion (in English) use at least 500 words,\n",
    "\n",
    "To have a more \"fluid\" comprehension of the steps done, I decided to structure the summary discussion into sections.\n",
    "\n",
    "# Preliminary Steps\n",
    "First of all, I started by doing some \"preliminary\" steps to \"take confidence\" in the dataset and the features present in it, in particular, I started by open the \".CSV\" file in Visual Studio Code and started looking for all possible noisy data within the dataset. As mentioned at the beginning of the notebook I found unnecessary newlines (\\n) in some row and a ',' too many in another row. After analyzing the rows I found some strings \" n.a.\" which I interpreted as missing values so as I will explain later I decided to replace them with NaN values. I have decided to delete the \\n and the ',' too many and save the clean version of the dataset as required by the delivery. Then I loaded the dataset using the pandas dataframe and in the read_csv function I specified the parameter na_values by passing it the strings \" n.a.\" in such a way that they become NaN.\n",
    "\n",
    "# Classification\n",
    "After the above steps, I started the real classification task by doing all the necessary steps to \"clean\" data, train different models, predict unseen samples, and then compare the results obtained from the various models with each other.\n",
    "## Pre-processing\n",
    "I started by doing some pre-processing steps, this is an important phase since data rarely are \"ready-to-use\" and so some steps are fundamental to make the learning algorithms work as best as possible to achieve better performances. In particular, in this phase I started by analyzing the missing values present in the Dataset, and I noticed that the total number of missing values were <= 10% of the total number of samples so I discarded them. Then I noticed that <i>url</i> feature was the only to have categorical feature so I decided to drop this columns because was not necessary and it thus also obtaining savings in terms of memory.\\\n",
    "Next, I discretized the target feature (shares) in 5 bins, the minimum reported by the delivery, as the authors of the report use two bins to discretize the target obtaining good results because they only had 0 and 1.\\\n",
    "The option that I decided to follow to handle the balancement of the classes was the SMOTE oversampling method, it is a better strategy with the respect to the downsampling and oversampling methods, since the first one may lead to discard useful samples and the second one may lead to overfitting. In particular, the SMOTE method creates new instances of the minority class by forming a convex combination of the neighboring samples.\\\n",
    "Once the classes were balanced I decide to normalize and scale the data, since some models, like SVM, are more efficient if data are scaled. Finally, to have a more understanding of the difference between using not normalized and scaled data I decided to save the normalized and scaled ones in a new Pandas DataFrame.\\\n",
    "I did not apply the encoding methods seen during the lab such as OneHot encoding, as all the features in the dataset were numeric.\\\n",
    "For Feature Importance I decided to fit another model to answer to this question, and then I analyzed the importance of each feature and I decide to discard the feature with importance <= 0.01 to have greater performance for some models.\n",
    "## Model Selection\n",
    "First of all, I split the dataset into two subsets, the training, and the test sets, which contained respectively 80% and 20% of the total samples. The first set, as the name suggests, was used to train the different models selected, and the test set was used to show the performance of each model on unseen samples i.e. test each model.\\\n",
    "The selected models were Decision Tree, SVM, Ensamble method: Random Forest, and MLPNs, and each model was trained on normalized and scaled data, and only for the decision tree I decided to train also with the original data for a matter of time, as I noticed that the other models with the unprocessed data turned out to be very slow.\\\n",
    "Obviously the execution time of the various models also depends on the number of parameters that are used to train, therefore for each model I have decided to use three parameters and to vary them as much as possible in order to obtain more satisfying results.\\\n",
    "In this phase for all the tested models was done some hyperparameter tuning via grid and randomized searching.\n",
    "## Evaluation\n",
    "In this phase I evaluated the results of each model on processed and not-processed data (I made this only for Decision Tree classifier as specified in the sections above). Once we have our model learned I analyzed the results obtained on the test set, in particular, the results were observed with the use of a confusion matrix and classification report. As I expected, the results obtained from processed data were better than the ones obtained from the not processed, in particular, I noticed that with processed data, Class 2 was really hard to predict, instead Class 0 was the \"best one predicted\" since all the measures associated to is were high. Now to have \"normalized\" results and to compare them with each other, I provide k-fold cross-validation, with k equal to 15, for each model. This was done to reduce the probability to be \"lucky\" or \"unlucky\" by splitting the dataset into training and test sets. After this finale operation, I compared the results with the max accuracy values and I obtained a higher accuracy with Random Forest classifier that reach arount 34%, followed by MLPNs classifier with an accuracy of 32% and ending with SVM and Decision Tree classifier with an accuracy of respectively 30% and 21%. I also tried to use 2 bins as the authors of the paper did and I got their same results for this reason I decided to print the max for the accuracy. But I also printed the average of the scores obtained +/- the standard deviation in such a way as to reduce the probability of being \"lucky\" or \"unlucky\" by splitting the dataset into training and test set. Therefore by analyzing these results I will obtain a higher accuracy with SVM classifier and MLPNs classifier that reach around 27%, and ending with Random Forest and Decision Tree classifier with an accuracy of respectively 24% and 12%. In the end, I can conclude that the Random Forest classifier reach the best accuracy if we consider the max accuracy but the value that matters is that of mean accuracy so I can conclude that the MLPNs and SVM classifiers reach the best accuracy and also the best results with the respect to the performances measured."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
